\section{Language}

Relay is a statically typed, purely functional, differentiable IR.
Relay is not a low-level IR intended for writing and optimizing high-performance
kernels; rather, it is intended to replace NNVM's computation graph as the input
layer of NNVM. We allow for primitive operators implemented either in external languages
such as C or C++ or in lower-level IRs like TVM or Tensor Comprehensions.
Because Relay is intended as the top layer of the TVM stack \cite{TVMSysML},
we have tight integration with TVM and use it to implement and optimize kernels.

Our intent is for our new IR to serve as a convenient means for researchers
to implement new differentiable programming languages and deep
probabilistic programming languages in the style of Edward and Pyro.

As we discussed in Section~\ref{background}, most popular machine learning
frameworks construct computation graphs that represent the user's program.
Since these graphs are essentially a modified form of an abstract syntax
tree (AST), we consider the transformations and analyses that have been
performed on computation graphs as program transforms and program analyses.
While other DL frameworks also adopt this perspective, their graph-based
approaches have made it difficult to bring the full arsenal of traditional
compiler and programming languages techniques to bear.

Static typing enables direct compilation of models into embedded hardware and
accelerators, which has been demonstrated in prior work done in the TVM
stack~\cite{TVMSysML}.  Having an IR like Relay enables the deployment of
richer dynamic models for applications such as natural language processing.  By
taking this point of view, we can leverage decades of programming language
research to help us express and understand these deep learning models not as a
restricted data flow language, but as a full programming language.

\begin{figure*}[t]
  \begin{minipage}{.5\textwidth}%
    \begin{grammar}
      <Item> ::= <Operator>
      \alt <Definition>

      <Operator> ::= \kwd{operator} <GlobalId> \kwd{:} <Type>

      <Definition> ::= \kwd{def} <GlobalId> (\kwd{(} <LocalId> : <Type> \kwd{)})* -> <Type> \kwd{\{} <Expr> \kwd{\}}

      <Expr> ::= <LocalId>
      \alt <GlobalId>
      \alt $\mathbb{R}$
      \alt \kwd{True}
      \alt \kwd{False}
      \alt <Expr> \kwd{(} (<Expr> (\kwd{,} <Expr>)*)? \kwd{)}
      \alt \kwd{let} <LocalId> \kwd{:} <Type> \kwd{=} <Expr> \kwd{in} <Expr>
      \alt \kwd{(} <Type> \kwd{)} <Expr>
      \alt <Expr> <BinOp> <Expr>
      \alt <UnaryOp> <Expr>
      \alt \kwd{(} (<Expr> (\kwd{,} <Expr>)*)? \kwd{)}
      \alt <Expr> \kwd{[} $\mathbb{N}$ \kwd{]}
      \alt \kwd{[} <Expr> (\kwd{,} <Expr>)* \kwd{]}
      \alt \kwd{if} <Expr> \kwd{then} <Expr> \kwd{else} <Expr>
      \alt \kwd{Zero} <Type>
      \alt \kwd{Grad} <Expr>
      \alt \kwd{Ref} <Expr>
      \alt \kwd{!} <Expr>
      \alt <Expr> \kwd{:=} <Expr>
      \end{grammar}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \begin{grammar}
    <BinOp> ::=
    \kwd{+}
    \alt \kwd{-}
    \alt \kwd{*}
    \alt \kwd{/}
    \alt \kwd{!=}
    \alt \kwd{=}
    \alt \kwd{\textless}
    \alt \kwd{\textless=}
    \alt \kwd{>}
    \alt \kwd{>=}

    <UnaryOp> ::=
    \kwd{-}
    \alt \kwd{sq}

    <Type> ::=
    <BaseType>
    \alt <Shape>
    \alt \kwd{Tensor} \kwd{(} <Type> \kwd{,} <Type> \kwd{)}
    \alt <Type> \kwd{->} <Type>
    \alt <TypeId>
    \alt \kwd{forall} \kwd{(} <TypeId> \kwd{:} <Kind> \kwd{)} \kwd{,} <Type>
    \alt \kwd{RefType} \kwd{(} <Type> \kwd{)}
    \alt \kwd{(} (<Type> (\kwd{,} <Type>)*)? \kwd{)}

    <BaseType> ::=
    \kwd{IntType} \kwd{(}  $\mathbb{N}$ \kwd{)}
    \alt \kwd{UIntType} \kwd{(}  $\mathbb{N}$ \kwd{)}
    \alt \kwd{FloatType} \kwd{(}  $\mathbb{N}$ \kwd{)}
    \alt \kwd{BoolType}

    <Shape> ::= \kwd{Shape} \kwd{(} ($\mathbb{N}$ (\kwd{,} $\mathbb{N}$)*)? \kwd{)}

    <Kind> ::=
    \kwd{BaseType}
    \alt \kwd{Shape}
    \alt \kwd{Type}

    \end{grammar}
\end{minipage}%
\caption{The BNF Grammar for the Relay langauge. Each case matches a node in our abstract syntax tree.
    References and related operations cannot be included in frontend user code and are only generated by the reverse-mode
    automatic differentiation.}
    \label{ref:lang_def}
\end{figure*}

\subsection{Grammar and Design}

The grammar for the full language can be found in Figure \ref{ref:lang_def}.

Relay is a functional language with closures, recursion, conditionals, operators,
and tensors. Relay's IR has two main design contributions over computation graphs:
the addition of functions and a rich type system that can capture the relationship
of tensor operations.

In order to support higher-order (in the sense of higher-order functions) differentiable
programs, we need to be able to support computing gradients over arbitrary
functions. We accomplish this by introducing a higher-order, higher-order (in
both senses) reverse mode operator \cite{toplas_reverse}. This operator allows
us to compute nth-order derivatives of higher order programs, opening up the
ability to differentiate over arbitrary control structures encoded with
functions.

Inspired in part by DLVM~\cite{dlvm}, a neural network DSL that supports a
CFG-style IR for deep learning programs which introduces a type system for
tensors that is based on constant tensor shape and types, Relay supports a rich
type system that includes dependent typing for tensor shapes, thereby allowing
function type signatures to specify the relationship between arguments (such as
attributes or other tensors) and the resulting tensor shapes.
