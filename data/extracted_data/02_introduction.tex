\section{Introduction} \label{introduction}

With the increased interest in deep learning in recent years, there has been an explosion of machine learning tools. Many popular frameworks such as Caffe~\cite{Caffe}, CNTK~\cite{CNTK}, TensorFlow~\cite{TF}, and Theano~\cite{Theano}, construct a static dataflow graph that represents the computation and which can then be applied repeatedly to batches of data. This approach provides visibility into the whole computation ahead of time, and can theoretically be leveraged to improve performance and scalability. However, it comes at the cost of ease of use, ease of debugging, and flexibility of the types of computation that can be represented. 

Prior work has recognized the value of dynamic eager execution for deep learning, and some recent frameworks implement this define-by-run approach, but do so either at the cost of performance (Chainer~\cite{Chainer}) or using a less expressive, faster language (Torch~\cite{Torch}, DyNet~\cite{DyNet}), which limits their applicability.

However, with careful implementation and design choices, dynamic eager execution can be achieved largely without sacrificing performance. This paper introduces PyTorch, a Python library that performs immediate execution of dynamic tensor computations with automatic differentiation and GPU acceleration, and does so while maintaining performance comparable to the fastest current libraries for deep learning. This combination has turned out to be very popular in the research community with, for instance, 296 ICLR 2019 submissions mentioning PyTorch.

\section{Background}

Four major trends in scientific computing have become increasingly important for deep learning.

First, starting in the 1960s, the development of domain specific languages such as APL \cite{APL}, MATLAB \cite{Matlab}, R \cite{R} and Julia \cite{Julia}, turned multidimensional arrays (often referred to as tensors) into first-class objects supported by a comprehensive set of mathematical primitives (or operators) to manipulate them. Separately, libraries such as NumPy\cite{Numpy}, Torch\cite{Torch}, Eigen\cite{eigenweb} and Lush\cite{Lush} made \textbf{array-based programming} productive in general purpose languages such as Python, Lisp, C++ and Lua.

Second, the development of \textbf{automatic differentiation}~\cite{autodiff_survey} made it possible to fully automate the daunting labor of computing derivatives. This made it significantly easier to experiment with different machine learning approaches while still allowing for efficient gradient based optimization. The autograd~\cite{maclaurin2016phd} package popularized the use of this technique for NumPy arrays, and similar approaches are used in frameworks such as Chainer~\cite{Chainer}, DyNet~\cite{DyNet}, Lush~\cite{Lush}, Torch~\cite{Torch}, Jax~\cite{jax} and Flux.jl~\cite{flux}.


%\quad 
Third, with the advent of the free software movement, the scientific community moved away from closed proprietary software such as Matlab\cite{Matlab}, and towards the \textbf{open-source Python ecosystem} with packages like NumPy \cite{Numpy}, SciPy \cite{SciPy}, and Pandas \cite{Pandas}. This fulfilled most of the numerical analysis needs of researchers while allowing them to take advantage of a vast repository of libraries to handle dataset preprocessing, statistical analysis, plotting, and more. Moreover, the openness, interoperability, and flexibility of free software fostered the development of vibrant communities that could quickly address new or changing needs by extending the existing functionality of a library or if needed by developing and releasing brand new ones. While there is a rich offering of open-source software for neural networks in languages other than Python, starting with Lush~\cite{Lush} in Lisp, Torch~\cite{Torch} in C++, Objective-C and Lua, EBLearn~\cite{EBLearn} in C++, Caffe~\cite{Caffe} in C++, the network effects of a large ecosystem such as Python made it an essential skill to jumpstart one's research. Hence, since 2014, most deep learning frameworks converged on a Python interface as an essential feature.
  
%\textbf{graphics processing units as a hardware accelerator} \quad 
Finally, the availability and commoditization of general-purpose massively parallel hardware such as GPUs provided the computing power required by deep learning methods. Specialized libraries such as cuDNN~\cite{cudnn}, along with a body of academic work (such as \cite{maxdnn} and \cite{fast_cnn}), produced a set of high-performance reusable deep learning kernels that enabled frameworks such as Caffe~\cite{Caffe}, Torch7~\cite{Torch7}, or TensorFlow~\cite{TF} to take advantage of these \textbf{hardware accelerators}.

PyTorch builds on these trends by providing an array-based programming model accelerated by GPUs and differentiable via automatic differentiation integrated in the Python ecosystem.


