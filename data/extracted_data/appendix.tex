\appendix

\section{Appendices}
\label{sec:appendix}

\subsection{Related Work in Other Fields}
\label{app:sec:relatedWork}
We survey the literature of adjacent fields and outline different streams related to our framework. These examples illustrate the ubiquity and complexity of bias, and highlight its understanding in different disciplines over time.

Bias became a crucial topic in social science following the seminal work of Tversky and Kahneman, showing that human thinking was subject to systematic errors~\cite{tversky1973availability}. Human logic was seemingly separate from the principles of probability calculus. ``Bias'' here is interpreted as the result of psychological heuristics, i.e., mental ``shortcuts'' to help us react faster to situations. 
While many of these heuristics can be useful in critical situations, their indiscriminate application in everyday life can have adverse effects and cause bias. 
This line of work has spawned an entire field of study in psychology (decision making). 

The focus of~\newcite{tversky1973availability} (and a whole field of decision making that followed) was human behavior. Still, the same basic principles of systematic differences in decision making apply to machines as well. 
However, algorithms also provide systematic ways to reduce bias, and some see the mitigation of bias in algorithm decisions as a potential opportunity to move the needle positively ~\cite{kleinberg2018discrimination}.
Thus, we can apply frameworks of contemporaries in human behavior to machines~\cite{rahwan2019machine}, and perhaps benefit from a more scalable experimentation process. \newcite{costello2014surprisingly} studies human judgment under uncertain conditions, and proposes that we \textit{can} algorithmically account for observed human bias, provided there is sufficient random noise in the probabilistic model.
This view suggests bias within the model itself, what we have called \textit{overamplification}.

Still, most works on bias in decision making assume working with unbiased data, even though social science has long battled selection bias. 
Most commonly, data selection is heavily skewed towards the students found on western university campuses~\cite{henrich2010weirdest}. 
Attempts to remedy selection bias in a scalable fashion use online populations, which in turn are skewed by unequal access to the Internet, but which we can mitigate through reweighting schemes ~\cite{couper2013sky}.

In some cases, algorithmic bias has helped understand society better. 
For example, \textit{semantic bias} in word embeddings has been leveraged to track trends in societal attitudes concerning gender roles and ethnic stereotypes. ~\newcite{garg2018word,kozlowski2018geometry} measure the distance between certain sets of words in different decades to track this change.
This use of biased embeddings illustrates an interesting distinction between \textit{normative} and \textit{descriptive} ethics. 
When used in predictive models, semantic bias is something to be avoided \cite{bolukbasi2016man}. I.e., it is \textit{normatively wrong} for many applications (e.g., we ideally would want all genders or ethnicities equally associated with all jobs). 
However, the works by \newcite{garg2018word} and \newcite{kozlowski2018geometry} show that it is precisely this bias of word embeddings that reflects societal attitudes. Here, the presence of bias is \textit{descriptively correct}. Similarly, \newcite{bhatia2017associative} uses this property of word embeddings to measure people's psychological biases and attitudes towards making individual decisions. 


\subsection{Discussion: Example Case Studies}
\label{sec:caseStudies}
%\todo{Andy: write out formally what is happening according to our framework (i.e. in Ps and Qs)}

\paragraph*{Part of Speech Taggers and Parsing.}
\label{appendix:case1}
The works by \newcite{hovy2015tagging, jorgensen2015challenges} outline the effect of selection bias on syntactic tools. The language of demographic groups systematically differs from each other for syntactic attributes. Therefore, models trained on samples whose demographic composition (e.g., age and ethnicity) differs from the target perform significantly worse.
Within the predictive bias framework, the consequence of this selection bias is an \textit{error disparity} -- $Q(\epsilon_{D = general}|A = {age, ethnicity}) \nsim Uniform$, the error of the model across a general domain ($D$) is not uniform with respect to attributes ($A$) age and ethnicity. 
\newcite{li2018towards} shows that this consequence of selection bias can be addressed by adversarial learning, removing the age gap and significantly reducing the performance difference between ethnolects (even if it was not trained with that objective).
\newcite{garimella-etal-2019-womens} quantifies this bias further by studying the effect of different gender compositions of the training data on tagging and parsing, supporting the claim that debiased samples benefit performance. 


\paragraph*{Image Captions.}
\label{appendix:case2}
\newcite{hendricks2018women} shows the presence of gender bias in image captioning, overamplifying differences present in the training data. Prior work focused on context (e.g., it is easier to predict ``mouse'' when there is a computer present). This bias manifests in ignoring people present in the image. The gender bias is not only influenced by the images, but also by biased language models. The primary consequence is an \textit{outcome disparity} -- $Q(\hat{Y}_D|gender) \nsim P(Y_D|gender)$, the distribution of outcomes (i.e. caption words and phrases) produced from the model $Q(\hat{Y}_{D}|gender)$ over-selects particular phrases beyond the distribution observed in reality: (i.e.  $P(Y_{D}|gender)$; this is true even when the source and target are the same: $D = source = target$). 

To overcome the bias and to increase performance, \newcite{hendricks2018women} introduce an equalizer model with two loss-terms: Appearance Confusion Loss (ACL) and Confident Loss (Conf). ACL increases the gender confusion when gender information is not present in the image, making it difficult to predict an accurately gendered word. Confident loss increases the confidence of the predicted gendered word when gender information \textit{is} present in the image. Both loss terms have the effect of decreasing the difference between $Q(\hat{Y}_D|gender)$ and $P(\hat{Y}_D|gender)$. In the end, the Equalizer model performs better in predicting a woman while still misclassifying a man as a woman, but decreasing \textit{error disparity} overall. 

\paragraph*{Sentiment Analysis.}
\label{appendix:case3}
\newcite{kiritchenko2018examining} show the issues of both semantic bias and overamplification. They assess scoring differences in 219 sentiment analysis systems by switching out names and pronouns. (They switch between male and female pronouns, and between prototypical white and black American first names based on name registers.) The results show that male pronouns are associated with higher scores for negative polarity, and prototypical black names with higher scores for negative emotions. The consequence of the semantic bias and overamplification are outcome disparities:  $Q(\hat{Y}_D|gender) \nsim P(Y_D|gender)$ and $Q(\hat{Y}_D|race) \nsim P(Y_D|race)$. 
This finding again demonstrates a case of descriptive vs.\ normative ethics. We could argue that because aggression is more often associated with male protagonists, the models reflect a descriptively correct (if morally objectionable) societal fact. However, if the model score changes based on ethnicity, the difference likely reflects (and amplifies) societal ethnic stereotypes. Those stereotypes, though, are both normatively and descriptively wrong. 

\paragraph*{Differential Diagnosis in Mental Health.}
\label{appendix:case4}
In the clinical community, differentiating a subject with post-traumatic stress disorder (PTSD) from someone with depression is known to be difficult. It was, therefore, surprising when early work on this task produced AUCs greater than 0.85 (this and similar tasks were part of the CLPsych2015 Shared task; \cite{coppersmith2015clpsych}).
Labels of depression and PTSD had been automatically derived from a convenience sample of individuals\footnote{A convenience sample, a term from social science, is a set of data selected because it is available rather than designed for the given task.} who had publicly stated their diagnosis in their profile. The task included a 50/50 split from each category.  
However, \newcite{preotiuc2015mental} show that these classifiers primarily picked up on differences in age or gender -- subjects with PTSD were more likely to be older than those with depression. 
While age and gender themselves are valid information for mental health diagnosis, the design yielded classifiers that predicted nearly all older individuals to have PTSD, and those younger to have depression, despite the 50/50 split. These classifiers resulted in \textit{outcome disparity}, because older individuals were much less likely to be labeled depressed than the target population (and younger less likely for PTSD:  $Q(\hat{Y}_{D}|A = age) \nsim P(Y_{D}|A = age)$).
In the end, the task organizers mitigated the issue by using matched controls -- adding another 50\% samples for each class such that the age and gender distributions of both groups matched. 
Recently, \newcite{benton2017multitask} showed that accounting for demographic attributes in the model could leverage this correlation while controlling for the confounds.

\paragraph*{Assessing Demographic Variance in Language.}
\label{appendix:case5}
A final case study in applying our framework demonstrates how inferring user demographics can mitigate bias. 
Consider the task of producing population measurements from readily available (but biased) community corpora. E.g., assessing representative US county life satisfaction from tweets~\cite{schwartz2013characterizing}. 
Unlike our other examples, the \textit{outcomes} of the source training data (i.e., surveys) are expected to be representative, while the \textit{features} come with biases. The source feature distributions with respect to human attributes are dissimilar from the ideal distribution, while the source outcomes match that target outcomes ($Q(X_{source}|A) \nsim P(X_{target}|A)$ but $Q(Y_{source}|A) \sim P(Y_{target}|A)$). 

In this case, the effectiveness of countermeasures preventing selection and semantic biases (for $X_{source}$ and $X_{target}$) should result in increased predictive performance against a representative community outcome. 
Indeed, \newcite{giorgi2019correcting} adjust the feature estimates, $X$, to match representative demographics and socio-economics by using inferred user attributes, and find improved predictions for the life satisfaction of a Twitter community. 
