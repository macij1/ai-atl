\subsection{Training Recipe}
\label{section:pretraining_training_recipe}
The recipe used to pre-train Llama 3 405B consists of three main stages: \textbf{(1)} initial pre-training, \textbf{(2)} long-context pre-training, and \textbf{(3)} annealing. 
The three stages are described separately below.
We use similar recipes to pre-train the 8B and 70B models.

\subsubsection{Initial Pre-Training}
We pre-train \llamathree 405B using AdamW with a peak learning rate of \SI{8e-5}, a linear warm up of 8,000 steps, and a cosine learning rate schedule decaying to \SI{8e-07} over 1,200,000 steps.
We use a lower batch size early in training to improve training stability, and increase it subsequently to improve efficiency. 
Specifically, we use an initial batch size of 4M tokens and sequences of length 4,096, and double these values to a batch size of 8M sequences of 8,192 tokens after pre-training 252M tokens.
We double the batch size again to 16M after pre-training on 2.87T tokens.
We found this training recipe to be very stable: we observed few loss spikes and did not require interventions to correct for model training divergence.

\textbf{Adjusting the data mix.} 
We made a several adjustments to the pre-training data mix during training to improve model performance on particular downstream tasks. 
In particular, we increased the percentage of non-English data during pre-training to improve the multilingual performance of Llama 3.
We also upsample mathematical data to improve the model's mathematical reasoning performance, we added more recent web data in the later stages of pre-training to advance the model's knowledge cut-off, and we downsampled subsets of the pre-training data that were later identified as being lower quality.


\subsubsection{Long Context Pre-Training}
In the final stages of pre-training, we train on long sequences to support context windows of up to 128K tokens. 
We do not train on long sequences earlier because the compute in self-attention layers grows quadratically in the sequence length.
We increase the supported context length in increments, pre-training until the model has successfully adapted to the increased context length. 
We assess successful adaptation by measuring whether \textbf{(1)} model performance on short-context evaluations has recovered completely and \textbf{(2)} the model perfectly solves ``needle in a haystack'' tasks up to that length. 
In Llama 3 405B pre-training, we increased context length gradually in six stages, starting from the original 8K context window and ending in the final 128K context window.
This long-context pre-training stage was performed using approximately 800B training tokens.

\subsubsection{Annealing} 
\label{section:annealing}

During pre-training on the final 40M tokens, we linearly annealed the learning rate to 0, maintaining a context length of 128K tokens. During this annealing phase, we also adjusted the data mix to upsample data sources of very high quality; see Section~\ref{sec:annealing_data}. 
Finally, we compute the average of model checkpoints (\citet{polyak1991averaging} averaging) during annealing to produce the final pre-trained model.
