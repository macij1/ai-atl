\section{Additional Safety Details}
\label{section:safetyappendix}

\input{appendices/lc_safety/lc_safety_app.tex}

\subsection{Assigning safety labels to model responses for internal benchmarks}
\label{section:safety_labels}
\textbf{Human evals}

For each prompt in the benchmark set, we obtain a response from Llama 3 and competitor models. We send these prompt-response pairs to human annotators for review. Annotators review the safety of the response, the (potential) violation type, the refusal type, and the response tone. Each prompt-response pair is reviewed by three annotators, the results are aggregated with the majority label being assigned (e.g., a response labeled by two raters as unsafe and one as safe will be labeled as safe). A 5\% sample of the data is sent to policy experts for accuracy review, with high disagreement cases prioritized. That is, unanimously labeled items are under-sampled and cases where human reviewers disagreed on at least one label are over-sampled. Any examples which are labeled incorrectly are used to educate and calibration human annotators ensuring continued data quality improvements. Datasets used for violation and refusal rate comparisons obtained high expert-annotator agreement rates ($\geq 90\%$) on the accuracy review sample.

\textbf{AI-assisted evals}

During the development of Llama 3 AI-assisted annotations have been used to complement human annotations used to guide decision making. While human annotations are still considered the golden standard, AI-assisted annotations (a.k.a. the LLM-as-a-judge paradigm) are becoming increasingly popular to improve annotation scalability in terms of time and capacity, or budget. In this approach an LLM is used as a judge model to evaluate responses of another LLM (e.g., \cite{kim2024prometheus2opensource,chiang2023largelanguagemodelsalternative}). Examples where AI-assisted annotations were leveraged during the development of Llama 3 include ablation studies and model checkpoint evaluations.

Specifically, AI-assisted annotations were leveraged for two types of evaluations. The first evaluation assessed the violation rates of different content risk types across different model capabilities and languages. Here the judge is asked to assess whether a prompt-response pair violated any of the content risk policies. The second evaluation quantified false refusal prevalence, in which case the judge is asked whether the response is a refusal while the prompt set is scoped to questions which should not be refused, making any refusal therefore false.

The basis of our AI-assisted annotation systems were the Llama 3 checkpoints themselves at various model sizes (8B, 70B and 405B parameters). Different approaches are typically used to create judges from such base models. The simplest option is to use a few-shot in-context learning approach where assessment guidelines and policies, as well as examples of evaluations, are passed directly in the system prompt. An alternative approach is to first fine-tune models with annotated data to improve performance, and then apply it using a zero or few shot approach. Finally, different enhancement methods such as a jury of models (ensembling), prompt engineering or a hybrid human and AI-assisted setup, can still be explored to further boost performance. During Llama 3 development the AI-assisted methods were optimised for different types of evaluations.

Finally, it is crucial to assess the performance of AI-assisted annotations against human annotations. Different techniques have been proposed in literature and several studies propose an in-depth analysis of judge performance across different use cases (e.g., \cite{huang2024limitationsfinetunedjudgemodels, zheng2023judgingllmasajudgemtbenchchatbot}). We present a judge built to assess violation rate of content risk standard for English text capability as well as a judge to assess false refusal rate. Cohen Kappa was used as a quantitative measure of reliability of two raters rating the same thing, while correcting for how often the raters may agree by chance \cite{cohen1960}.

\begin{table}[h!]
\centering
\begin{tabular}{||c | c | c ||}
 \hline
 Task - Judge & Baseline & Hybrid setup \\
 \hline\hline
 English short context violation rate - Few Shot approach on Llama 3 405B & 0.59 & 0.90  \\
 English short context false refusal rate - Zero Shot approach on Llama 3 8B & 0.86 & -  \\
 \hline
\end{tabular}
\caption{Performance of AI-assisted annotations against human annotations golden dataset. Cohen Kappa metric. Hybrid setup refers to the case where judge predictions with low confidence are filtered and sent for human annotation (6.5\% humans, 93.5\% judge).}
\label{table:judge-performance}
\end{table}


In Table \ref{table:judge-performance} we observe that a zero shot judge based on Llama 3 8B model assessing false refusal achieves a Cohen Kappa of 0.88. This value is in the range of almost perfect agreement: hence this judge can be used to obtain an estimate of false refusal rate very close to human annotators.  Assessing violation rate appears to be a more complex task. Using a few shots approach based on Llama 405B for this task achieves a Cohen Kappa of 0.59 that is considered in the range of substantial agreement. Annotations obtained via this judge can be used to rank models, e.g., compare different model checkpoints or versions. We observe that using an hybrid human and AI-assisted setup, where judge predictions with low confidence are filtered and sent for human annotation, allows achieving a Cohen Kappa of 0.9, that is in the range of almost perfect agreement. This approach allows producing estimates of almost the same quality of fully human annotated samples with only 6.5\% of samples actually labelled by humans and the remaining 93.5\% by the judge.


\subsection{Additional Speech Safety Results}

Table~\ref{table:speech-safety-mutox-all} shows the detailed added and lost toxicity percentages across the 21 languages we evaluated on.



\providecommand{\bup}{($\boldsymbol\uparrow$)}
\providecommand{\bdown}{($\boldsymbol\downarrow$)}


\begin{table} 
\centering
\resizebox{\linewidth}{!}{
    \begin{tabular}{ l | c | c | c | c|c|c }
    \toprule
    \textbf{Language} & \textbf{Llama 3 8B AT \bdown} & \textbf{Llama 3 8B LT \bup} & \textbf{Llama 3 8B AT \bdown} & \textbf{Llama 3 8B LT \bup} & \textbf{AT Gemini \bdown} & \textbf{LT Gemini \bup} \\
    \midrule
    English & 0.84 & 15.09 & 0.68 & 15.46 & 1.44 & 13.42  \\
    Arabic & 2.40 & 4.45 & 2.82 & 4.47 & 1.12 & 6.95 \\
    Bengali & 0.54 & 2.45 & 0.42 & 2.42 & 0.52 & 2.55 \\
    Chinese & 0.60 & 8.40 & 0.47 & 8.55 & 1.07 & 8.30 \\
    German & 3.82 & 13.97 & 3.40 & 12.85 & 2.65 & 18.47 \\
    Greek & 3.52 & 6.67 & 2.22 & 7.00 & 2.60 & 7.12 \\
    Finnish & 4.90 & 8.12 & 4.15 & 8.70 & 2.90 & 8.22 \\
    French & 3.05 & 5.25 & 2.62 & 5.22 & 2.92 & 5.85 \\
    Hindi & 1.00 & 10.00 & 0.47 & 10.45 & 0.62 & 10.15 \\
    Hungarian & 4.05 & 10.67 & 3.77 & 10.40 & 1.84 & 10.25 \\
    Indonesian & 1.52 & 7.30 & 0.89 & 8.05 & 1.05 & 8.47 \\
    Italian & 4.12 & 9.37 & 3.45 & 9.72 & 3.12 & 10.10 \\
    Persian & 1.52 & 4.67 & 1.25 & 4.97 & 1.55 & 4.70 \\
    Polish & 5.92 & 6.10 & 5.97 & 6.10 & 3.87 & 7.55 \\
    Portuguese & 2.60 & 10.80 & 1.70 & 11.75 & 3.17 & 10.87 \\
    Russian & 3.32 & 6.10 & 3.80 & 5.94 & 2.42 & 8.00 \\
    Spanish & 2.81 & 12.49 & 2.35 & 13.64 & 3.52 & 12.86 \\
    Swahili & 0.70 & 5.40 & 0.77 & 5.32 & 0.65 & 5.37 \\
    Turkish & 2.00 & 4.85 & 1.95 & 4.77 & 0.75 & 6.37 \\
    Urdu & 0.52 & 17.67 & 0.60 & 17.52 & 0.54 & 17.97 \\
    Vietnamese & 2.72 & 6.77 & 2.07 & 8.77 & 1.52 & 10.92 \\
    \bottomrule
    \end{tabular}
}
    \caption{Speech toxicity of LLM output for different languages on MuTox dataset. AT refers to added toxicity (\%) and LT refers to lost toxicity (\%). \label{table:speech-safety-mutox-all}}
\end{table}
