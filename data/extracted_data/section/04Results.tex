\section{Experimental Investigation}
\label{sec:results}

\subsection{Overview}
\label{sec:res-overview}
The purpose of this work is to explore the potential for general-purpose language models in translating system specifications provided in English to their hardware implementations in the Verilog \ac{HDL}.
As such we are interested in measuring the quality of the generated Verilog.
This raises an obvious question---how should one define "quality"? 
%Recall that 
In this work we are interested in a language model which can perform design tasks of a similar difficulty to those posed in a textbook~\cite{vahid_digital_2010}.
% As such, we are presenting Task/Result pairs similar to these kinds of questions to the language model for learning with the goal that GPT-2 can learn to solve related challenges.

% In a classroom setting, "marking" the design is done by a human with domain knowledge able to quantify the quality/correctness of the design.
% Their goal is primarily to measure the \emph{understanding} and skill level of the response-writer, with at least partial credit for "almost" correct attempts. 
% However, as there are no current automated systems to convert English specifications into Verilog, 
However, there are no automated systems to quantify how well a specification has been implemented in its corresponding Verilog if it is "almost" correct. 
Formal equivalence check is an option, but requires that the design is at least syntactically compliant. 
This presents a challenge as we wish to quantify the quality of \sol's Verilog generation. 
However, given that we generate Task/Result pairs with a template engine, we have a baseline `canonical' response that we can compare \sol's output against.
This allows us to introduce the equivalence between the two generators as a measure of quality, discussed in \autoref{sec:equality}. 
Where \sol's output is not equivalent, we manually examine the result qualitatively. 

An important part of our evaluation is to examine \sol's performance over unfamiliar texts.
Otherwise, it could be argued that the language model has simply learned a kind of pattern recognition over the Task/Result pairs, and is just using string relocation techniques to score highly during validation.
If this notion were applied to a student, we might say that they had learned to produce Verilog by rote, rather than through understanding. 

This examination is provided through the Non-Trained Templates. 
Recall that these are unfamiliar to \sol, i.e., they were not seen during fine-tuning,
and \sol~ has had no opportunity to learn/memorize their syntax and structure.
%
%The purpose of these templates is to provide insights into \sol's performance over unfamiliar texts. 
We seek insight from \sol's performance over these tasks as evidence that the GPT-2 language model offers promise for our intended translation purpose.
%Without these templates it 
%

% The quality of the outputs of the Non-Trained Templates are the counter to this argument, as \sol~ was provided no opportunity to learn their syntax and structure.
% It is these that will provide the greatest evidence that the general GPT-2 language model may be used for our intended purpose.

% In the rest of this section 
% We now provide insights from our investigation in three parts: \sol's performance on well-defined, prescriptive tasks (\autoref{sec:translation-prescriptive}), descriptive tasks (\autoref{sec:translation-descriptive}), and multi-tasks (\autoref{sec:results-multi-tasks}).
% Firstly, we examine the trained model's performance on prescriptive---well-defined---tasks. This is presented in \autoref{sec:translation-prescriptive}.
% Secondly, we examine the model's capability in more abstract settings, over descriptive and less well-defined tasks. This is presented in \autoref{sec:translation-descriptive}.

%\todo{Something here about predictive (accuracy) vs. generative (quality/distance) metrics. In NLP...}

\subsubsection{A measure of equality.} 
\label{sec:equality}
There are numerous ways to implement a given specification in any programming language. 
Take the example from \autoref{fig:task-result-gen-process}: while it provides the correct answer as \texttt{assign c = a~|~b;}, it could be equivalently specified as \texttt{assign c = b | a;}.
This becomes even more of an issue when implementing larger and more complex and descriptive specifications.

While there are ways of quantifying identical code (e.g., comparing abstract syntax trees), we opt, for a simpler comparison of \sol's outputs against the template tool using a sequence equivalence metric. This is because the generated Verilog code should be relatively short and simple.
More precisely, we define \textit{correctness} of the generated text as its distance to the template-provided ``correct'' answer (excluding white-space characters from both) as measured by their Ratcliff-Obershelp similarity~\cite{ratcliff1998ratcliff}.
This means that if \sol~ returns \texttt{assign c = a | b;} as the correct answer to the prompt in \autoref{fig:task-result-gen-process}, it scores $1.00$---i.e., the result is fully correct.
However, despite being functionally equivalent, a result of \texttt{assign c = b | a;} scores only $0.833$.

While this metric is simple, %for the purposes of the experiments laid out in \autoref{sec:dataset-prep}, there are few opportunities for \sol~ to perform large-scale restructures of code such that they are functionally equivalent yet syntactically distinct. 
% In addition, 
manual inspection of the results that did not have the expected score of $100$, revealed no examples where \sol~ had performed small but functionally equivalent changes (e.g., inverting the order of variables compared to their order in the specification). That the output has a deterministic ordering to the variables is not a surprising result, as the template engine that \sol~ is fine-tuned from has a deterministic order to the Verilog code that it produces. We provide insights from our investigation in three parts: \sol's performance on prescriptive (\autoref{sec:translation-prescriptive}), descriptive (\autoref{sec:translation-descriptive}), and multi tasks (\autoref{sec:results-multi-tasks}).

\subsection{Translation of Prescriptive Specifications}
\label{sec:translation-prescriptive}

\input{results/prescriptive}

\sol's performance on prescriptive tasks is presented in \autoref{tbl:res-prescriptive}, with Non-Trained templates highlighted in \textbf{bold}. Each row contains information on the number of template samples used for fine-tuning, the number of template samples used for validation, the number \sol~ returned correctly, and (where applicable) the average Ratcliff-Obershelp (R-O) similarity of returned incorrect answers compared to the correct answer.

With regards to assignments, \sol~ performs  well on tasks based on Trained (e.g., \textit{pa00}\footnote{\textit{pa00} example: ``Put the result of `a' nand `b' in `c'.''}) templates, getting 99.7\,\% of all samples correct across this validation category.
It performs slightly worse on tasks drawn from Non-Trained templates (e.g., \textit{pa18}\footnote{\textit{pa18:} ``Assign into output `c' the result of `a' xor `b'.''}), scoring 96.5\,\% correct.
\sol~ scores well on Trained register templates (e.g., \textit{pr00}\footnote{\textit{pr00:} ``Define a 8-bit register `a' with input `a' defined as `b' and `c', enable `e', and clock `c'.''}) (99.2\,\% correct).
Likewise \sol~ performed well with the Non-Trained Templates in this category (e.g. \textit{pr11}\footnote{\textit{pr11}: Given input 'a', enable 'e' defined as 'd' nxor 'f', an asynchronous reset 'r' (being 'x' or 'y') make a 7-bit register 'q'.}), with 98.7\,\% correct.
While \sol~ did well in Trained Sequence Generators (e.g. \textit{pg01}\footnote{\textit{pg01:} ``Define sequential code which will produce the repeating sequence [00, 10, 10] on the 2-bit output `q'. It should advance on each tick of a clock `c' whenever enable defined as `a' nxor `b' is present.''}) with 99.5\,\% correct across the samples, it performed poorly with the Non-Trained template \textit{pg06}\footnote{\textit{pg06:} ``Produce a design that generates a 3-bit output `uy' with the sequence: [110, 100, 101, 100]. The output changes with each rising edge of a clock if the enable signal `a' less than `b' is asserted. Whenever an asynchronous reset `r' is asserted, the design should output the first element of the sequence.''}, bringing the overall percentage correct for Non-Trained Templates down to 85.6\,\%.
 
% It is worth examining why exactly certain templates under- and over- perform. 
\textbf{Discussion.} 
One would expect \sol~ to perform well on tasks produced from Trained templates, given that % it is expected that they would validate with a high percentage of accuracy as \sol~ is
these most resemble the training data. %fine-tuned over examples generated from their template structure.
This held true for all three major categories.
One might also expect that \sol~ would perform worse on task prompts generated from Non-Trained templates in comparison to prompts generated from the Trained templates. 
% Our expectation with the Non-Trained templates was that \sol~ would perform worse than over the trained counterparts. 
Our hypothesis is that the GPT-2 pre-training should allow \sol~ to generalise and produce the correct Verilog even in unseen tasks. % for the Non-Trained templates.

This holds for Assignments and Registers, but did not entirely hold with the Non-Trained Sequence Generator templates, specifically with \textit{pg06}.
Closer investigation of this template revealed that almost all of \sol's errors (>95\,\%) stem from mis-classification of enable and reset signals. This was unexpected as \sol~ did not have this issue over tasks based on any other Sequence Generator template.
One theory is that the issue may stem from the difference between \textit{pg06} and the other templates---perhaps it is \emph{too} unique. 
To evaluate this, we compared the the R-O similarity of templates \textit{pg05} (which scored 100\,\%) and \textit{pg06} with the Trained \textit{pg} templates.
We found that \textit{pg05} was closest to \textit{pg01} (similarity $0.820$), whereas \textit{pg06} was closest to \textit{pg03} (similarity $0.777$).
These numbers are similar enough that we would have expected \textit{pg06} to score better. Further formal analysis is an avenue for our future work. It is likely that providing a greater variety of Sequence Generator templates during training would help \sol~ produce more accurate results.

%This is reflected in the high \emph{Avg. Error Accuracy} column, i.e., high similarity between \sol's output and the "canonical" Verilog. 

%As mentioned in \autoref{sec:res-overview}, it is the performance of GPT-2 over the unfamiliar Non-Trained templates that provides the most evidence that GPT-2 can be used as a translation tool.
%\todo{Update based on latest results}
%\textit{cr14} and the other \textit{crXX} templates, revealing the closest matching template was \textit{cr00} with a score of $90.1$. As such, we would have expected \textit{cr14} to perform better.

%When creating Sequence Generators, \sol's response to tasks based on Non-Trained Templates is of similar quality to those for tasks based on Trained Templates; we observe similar error patterns (inaccuracy in the Sequence output compared to the prompt).

\input{results/descriptive}
\subsection{Translation of Descriptive Specifications}
\label{sec:translation-descriptive}

\autoref{tbl:res-descriptive} presents \sol's performance over Descriptive Tasks. 
While this category has fewer templates, each template has more opportunities for entropy due to the presence of optional clauses and implicit intermediate signals. % generation.
We also design these templates to be more ``difficult''---they invoke requirements such as `active-high' and `active-low' qualifiers to their variables, terms that \sol~ needs to recognise and accommodate in the generated Verilog.

Somewhat surprisingly, \sol~ performs better on Descriptive Tasks than on the Prescriptive Tasks, with 99.2\,\% correct Assignments and 99.0\,\% Registers over the Trained Templates.
For the Non-Trained templates, the Assignments scored 100\,\% correct and Registers scored 98\,\%.
To check that this high score was not due to the Non-Trained templates \textit{da03} and \textit{dr04} being structurally similar to the Trained templates, we compare R-O similarities. \textit{da03} is most similar to \textit{da01}, with a score of $0.686$. \textit{dr04} is most similar to \textit{dr02}, with a score of $0.703$. 
While these values might seem high, consider the  Sequence Generator template \textit{pg06}, which scored $0.777$ yet \sol~ gave the correct answer only 71.5\,\% of the time.

\textbf{Discussion.} 
On a number of occasions, we were particularly impressed that \sol~ was able to derive the Boolean combinations for certain operations.
Take this example from \textit{da00}: ``A car has four active-low door open sensors `a', `b', `c', `d'. Write combinatorial logic for a active-low light `l' which illuminates when any of the doors are open.''
From that prompt, \sol~ is able to correctly generate the output \texttt{assign l = a \& b \& c \& d;}, i.e., it appears to associate `any' and `doors', as well as understand the relationship between `any' and the two `active-low' qualifiers.
Another example of \sol~ "understanding" keywords is the generated Verilog for \textit{dr00}, which we present in \autoref{fig:example}. % along with its generated output. 
\sol~ can correctly implement both synchronous and asynchronous resets, as well as infer clocks for memory elements when no clocks are explicitly specified.

\subsection{Translation of Multiple Tasks}
\label{sec:results-multi-tasks}

For insight into how \sol~ can handle the processing of multiple tasks simultaneously we also provided a multi-task metastructure consisting of 2-4 registers and assignments in a single Task prompt.
These are presented in \autoref{tbl:res-descriptive} under M-T.
We divide Multi-tasks into two broad categories---those made purely from Trained templates (of which 5000 were presented during the fine-tuning process), and those made only from Non-Trained templates.
Multi-tasks performed worse than the individual templates (Trained correct 52\,\% of the time, and Non-Trained 41.2\,\%).
Upon manual inspection, \sol~ was generating the correct Verilog structures and syntax in the outputs, usually only getting variable names/operators incorrect. This is reflected in the Average Error R-O, which is high given the answer lengths. It is likely that the difficulties \sol~ is facing with multi-tasks stem from the na\"{i}ve concatenation of tasks. In future we will explore multi-tasks where the "sub-tasks" are related. 

% {\bf: FIXME: haphazard implementation of the metastructure, which concatenates tasks that have no relation to one another.} 
%As such, we believe that with more training examples built to implement tasks that bear relation to one anther, \sol~ could learn to produce higher quality outputs.

\subsection{Discussion and Limitations}

The results presented are promising.
\sol~ has shown clear ability to produce syntactically correct Verilog (in our tests, it rarely, if ever, produced outputs that could not compile---errors were almost always related to operator choice and/or variable names).
\sol~ is capable of producing code with complex relationships between inputs and outputs, and even with intermediate signals. In total, \sol~ returned the correct answer in \textbf{94.8\,\%} of all validation tests.

That said, our work has limitations.
Firstly, other than inferring clocks, we do not yet ask \sol~ to create a signal that was not already named or otherwise described (e.g., we never provide code such as "Output `a' nor `b'", it is always "Output `a' nor `b' \underline{in `c'}.").
Likewise, we never rely on any form of \emph{creativity} in the generated results---our training data suggests that there %the intention from our templates was that there should
is only one path forward to the implementation for a given task template. 
That is, our templates had a \emph{many-to-one} relationship with the Verilog they described, despite there being different ways to express functionally identical Verilog. % (as previously discussed). % in multiple different ways.
%This does mean that there are simple prompts that the engine cannot process \todo{check this: e.g., ``Generate a 7-bit register.'' will fail, whereas ``Generate a 7-bit register called 'q'.'' will succeed}.
These are the focus of our ongoing  studies.%, we will examine broader task classes. 

\sol~ inherits some \emph{technical} limitations of GPT-2: The model can only generate outputs of up to 1024 tokens (i.e., words, symbols). As longer snippets of code can potentially run into this limit, we had to limit certain inputs---sequence generators were capped at no more than 4 elements, and our multi-tasks were prevented from using long-winded descriptive register templates.