\section{Fine-tuning GPT-2 for Verilog}
\label{sec:experiment}

\subsection{Problem definition}

In this work, we focus on an early-stage \ac{CAD} problem: interpreting a high-level, informal description of functionality and producing the corresponding concrete specification. 
For small designs, designers can craft an RTL specification directly after identifying the necessary inputs, outputs, and the relationships between them from a short description of a task.
% For example,~\autoref{fig:example} shows a short plain language description (the "task") and Verilog implementation (the "result"). 
While previous works use algorithmic approaches such as parse-tree generation and sub-tree matching~\cite{zhao_automatic_2019} to identify the salient elements of the natural language description for populating templates, we re-cast the problem holistically as \textit{translation}.
% Taking inspiration from Radford \textit{et al.'s} finding that GPT-2 could "learn" to translate without explicit instruction, we explore GPT-2's ability to produce Verilog snippets after transfer learning on dataset of natural language--Verilog pairs (i.e., {\it Task/Result} pairs). 
As we describe next, we prepare examples of task descriptions with varying descriptiveness, and examine GPT-2's ability to produce Verilog after transfer learning~\cite{pan_survey_2010}. 
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/paper-gpt2-verilog-template-engine.pdf}
    \caption{The Task/Result Generation Process \label{fig:task-result-gen-process}}
    \vspace{-5mm}
\end{figure}

\subsection{Dataset Preparation}
\label{sec:dataset-prep}

\input{results/templates}

%\todoblock{I'm not sure if this is "labelling"---it's a little bit more of targeted "task-specific input transformations" or domain data...maybe rework? It's the same sort of learning problem in the sense that internally, it must be learning to produce the new sequence?}
% Since our pre-trained model was trained on contiguous sequences of text, we require some modifications to apply it to these tasks. Previous work proposed learning task specific architectures on top of transferred representations [44]. Such an approach re-introduces a significant amount of task-specific customization and does not use transfer learning for these additional architectural components. Instead, we use a traversal-style approach [52], where we convert structured inputs into an ordered sequence that our pre-trained model can process. -- From Radford et al. 2018

% Transfer learning enables a model that has been pre-trained on data from one domain to be fine-tuned with data from a target domain~\cite{pan_survey_2010}. 
% These fine-tuning datasets, unlike the data used earlier in the pre-training process, are domain-specific and may be structured in order to enable supervised learning.

% In our case, we wish to 
In this work, we fine-tune GPT-2 to produce \sol, aiming for the ability to translate natural language (i.e., English) into Verilog. 
GPT-2 is designed to process contiguous text sequences, so we adopt the approach proposed in~\cite{radford_improving_nodate}, to represent the English--Verilog translation task as an ordered sequence 
% As such, we will require substantial quantities of data consisting of pairs of English language tasks and their equivalent Verilog code, 
in the format \texttt{`TASK: <English Text> RESULT: <Verilog Code>'}.

% While significant quantities of 
Open-source Verilog code can be found online, but is unstructured, with varying quality and complexity. 
% While fine-tuning allows one to adapt pre-trained models with fewer labeled samples than training from scratch, we decided that 
% In order to get selected samples into the required format for fine-tuning, 
% significant human effort in trawling, parsing, and documenting code from the wild. 
% it would be required.
% As such, we instead rely on a
For this initial study, we design a custom dataset generation tool inspired by the sort of template-based, random auto-marking \emph{Q\&A} systems used in teaching settings (e.g., the OASIS Question Engine\footnote{https://www.oasisqe.com/}).
Rather than produce thousands of Task/Result pairs manually, we prepare several natural language \emph{templates} which encapsulate different task scenarios. % are instead produced and stored in a \emph{template repository}.
% Using these templates our tool can then randomly generate its own Task/Result pairs for training.
An example generation process is shown in \autoref{fig:task-result-gen-process}.

In step (1) our tool generates a Task/Result \emph{metastructure}, a descriptor for the type of task (e.g., an assignment) and relevant information for that task (e.g., variable names, operators).
Possible metastructure tasks include combinational signal \emph{assignments}, \emph{registers}, \emph{sequence generators}, or a multi-set of these. 
Then, in step (2), the tool randomly chooses a suitable template for the task that encapsulates all information in English and Verilog.
In step (3), the tool ``fills in'' these templates, translating arguments where necessary (e.g. \texttt{OR} operator is `\emph{or}' in English and `\texttt{|}' in Verilog). Finally, in step (4), the tool saves the generated Task/Result pair. % into the dataset.

Structurally, we organise our templates into the different task classes they describe---(combinational) assignments, registers, and sequence generators.
We then categorise them further as either \emph{prescriptive} or \emph{descriptive}. 
\textbf{Prescriptive templates} are like the example presented in \autoref{fig:task-result-gen-process}. 
We conjecture that these should be trivial to translate---simple substitutions and word-reordering is all that is required to convert from the English to Verilog.
\textbf{Descriptive templates}, meanwhile, are more like the example presented in \autoref{fig:example}. 
They are more complex to translate, and a human designer would implicitly perform intermediate steps---such as understanding that a given input is being used as an enable signal or as a reset.
\textbf{Multi-task} templates are random concatenations of two to four assignment/register templates. 
\autoref{tbl:dataset} provides additional examples of the different task types generated from the various templates. 

While at first glance this template-based approach for dataset generation might appear to restrict \sol's ability to generalize over English descriptions, this dataset is only used for \emph{fine-tuning} the language model. As GPT-2 is pre-trained over the large WebText dataset~\cite{radford2019language}, we theorize that \sol~ should retain at some ability to process natural language features such as  synonyms and different word/clause orders. To validate this hypothesis, we hold-out a subset of templates for use during testing and evaluation. 
\autoref{tbl:dataset} has information about the final dataset, including the number of "Trained" and "Non-Trained" (held-out) templates for all task types. 
% one final time into two additional categories---\emph{Trained} and \emph{Non-Trained}. 

% Trained templates are used in both training and validation and represent the baseline ability of GPT-2 to translate familiar text structures into Verilog.

In our evaluation, we initially query \sol~ with new task instances based on Trained templates to observe its baseline ability to perform "familiar" tasks (i.e., produce Verilog from English descriptions that are similar to the training data). 
% Non-Trained templates are used 
To study generalizability of the approach, we query \sol~ with new task instances based on Non-Trained templates, i.e., such Task/Result pairs %generated from Non-Trained templates 
are presented to the language model during validation.

% \autoref{tbl:dataset} contains information about the final dataset. 
% Note that w
While the number of templates might appear low in certain cases (e.g., \# of Descriptive vs. Prescriptive assignments), the task instances of the given templates vary significantly from each other due to the addition or omission of optional clauses in the natural text during data generation.  A template that describes a register design task may have a clause describing a reset signal, and if the template is used for a metastructure with no reset signal, that entire clause is omitted.
As such a given template identifier refers only to the overall sentence structure used in a Task, the unique pattern of compulsory words within that template, such as introductory remarks (e.g. ``Describe combinatorial logic to...''), and individual words used within that template (e.g. conjunctions, prepositions).
Descriptive templates have randomly generated \emph{settings} such as ``an attendant call button''. These are generated from the cascaded sub-templates, increasing the entropy of each individual Task/Result pair.
Register and Sequence Generator templates are allowed to recursively define the basic template (prescriptive assignments).
A register might define a signal (e.g. an enable) as a function (e.g. `a' nand `b') rather than as a pre-set input (e.g. `c').

Multi-tasks combine other types of tasks and are difficult to categorise. We randomly generate 5,250 multi-task samples, of which 5000 are used for fine-tuning. We discuss details in \autoref{sec:results-multi-tasks}.

\subsection{Experimental Platform}
After we generate a suitable corpus of Task/Result pairs according to the method described in \autoref{sec:dataset-prep}, we fine-tune the 345 million parameter GPT-2 model on a high-performance computing node with 2 Intel Xeon E5-2698 v4 @ 2.20GHz cores, 20~GB of RAM, and an NVIDIA V100 32~GB graphics card over all categories of Task/Result pairs simultaneously (i.e. the same trained model is used to decode each type of Task). Our fine-tuning script is modified from \cite{aitextgen}. 
We use the Python programming environment, with \emph{pytorch} version 1.5.0, \emph{tensorflow} version 2.2, and \emph{aitextgen} version 0.2.3. Underlying these we use \emph{cuda} and \emph{cudnn} version 10.1.

To fine-tune GPT-2, we leave the hyper-parameters at their suggested defaults (\emph{learning rate} 1e-4, \emph{weight decay} 0.05, \emph{adam epsilon} 1e-8) and perform fine-tuning for 7500 steps.
The training data covers a random sample of 95\% of the generated samples of each Trained template category, with 5\% held back for evaluating the model.
%For example, for a given template in the \textit{Prescriptive Assignment} category, we generate 2000 samples (with randomly created variable names and operators). We use 1900 for fine-tuning, 100 for validation.
%For Non-Trained templates, we  use 5\% of the generated samples for validation. %, with the other 95\,\% discarded.
To evaluate model "goodness", we use the same computing resources as for training and use default GPT-2 output generation parameters (\emph{temperature} 0.7, \emph{top\_p} 0.9, and \emph{top\_k} 0/disabled). 
