\section{Introduction}
\label{sec:intro}

\begin{figure}[b]
\begin{tcolorbox}[fontlower=\footnotesize, fontupper=\footnotesize,top=3pt,bottom=2pt,left=3pt,right=3pt,]
{TASK: Write sequential code for a call button (e.g., in an airplane or hospital). If the call button 'b' is pressed (= 1) then the call light 'l' should turn on (= 1).
The output call light 'l' should turn off (= 0) when the synchronous cancel button 'r' is pressed (= 1). }\\ \vspace{-4mm}
\tcblower  \vspace{-2mm}
{RESULT:}
\begin{lstlisting}
// assume clock clk
reg l;
always @(posedge clk) begin
 if(r) begin
  l <= 0;
 end else if(b) begin 
  l <= 1;
 end
end
\end{lstlisting}
\vspace{-7mm}
\end{tcolorbox}
\vspace{-4mm}
\caption{Example "Task" and Corresponding Verilog\label{fig:example}}
\vspace{-10mm}
\end{figure}

In pursuit of simplifying and acceleration digital design, a machine-driven design flow with ``no humans in the loop'' is a long-term goal of projects such as OpenROAD\footnote{https://theopenroadproject.org/}.
Typically, the starting point is human-prepared hardware specifications in a \ac{HDL} such as Verilog.
However, manually producing \ac{HDL} to match a given specification (e.g. in \autoref{fig:example}) requires significant domain knowledge and is challenging to write error-free. 
As such, there is an opportunity for automatic translation to increase productivity and reduce the burdens on human designers. 
Given successful adoption of \ac{ML} throughout the \ac{IC} \ac{CAD} flow (e.g,~\cite{servadei_accurate_2019,yu_developing_2018,kahng_machine_2018}), we are motivated to investigate if state-of-the-art \ac{ML} can help in even earlier design stages. 

\ac{ML} has recently made great strides in \ac{NLP}. 
Advances in \ac{DL} have included new architectures such as LSTMs~\cite{sundermeyer2012lstm}, RNNs~\cite{liu2016recurrent}, and Transformers~\cite{vaswani_attention_2017}. 
These architectures have led to models such as BERT~\cite{devlin2018bert} and GPT-2~\cite{radford2019language} which demonstrate capability in language modelling, language translation (e.g., English to French), reading comprehension/understanding (e.g., answering questions from the CoQA~\cite{reddy2019coqa} dataset), and information storage/retrieval. 
In fact, GPT-2 made headlines~\cite{hern2019new} for initially being "too dangerous" to release given the "quality" of its text generation. 
%That said, \todo{there has been some work thus far in generation of code using NLP for NLP}.
%However, \todo{limitations}.
\emph{Can we harness this power to produce hardware from task descriptions (like in~\autoref{fig:example})?} 

Towards the goal of fully automated design from natural language, we investigate the adaptation of a pre-trained natural language model to perform English to Verilog "translation". % after training on natural text. 
Using transfer learning~\cite{pan_survey_2010}, we fine-tune the recently presented GPT-2 for this task by training it on a custom dataset of Task/Result pairs, as in~\autoref{fig:example}. 
The tasks are somewhat akin to novice-level "textbook" problems (i.e., similar to those found in a classic textbook~\cite{vahid_digital_2010}).
We validate our approach by presenting a set of "unseen" tasks to translate and measure the quality of output. %the output GPT-2 is able to produce.
% This study thus presents the following contributions.
Our contributions are:
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
\item \sol, a pre-trained GPT-2 model that can translate natural language into Verilog implementation.
\item A method to automatically generate a large quantity of English specification, Verilog pairs for fine-tuning \sol.
\item Exploration and evaluation of fine-tuning \sol.% so that it can translate natural language into Verilog implementation for simple specifications. 
\item Rating ~\sol~ in translating complex \emph{descriptive} tasks besides those presented in simple prescriptive forms.
\end{itemize}
The rest of the paper is as follows. 
\autoref{sec:related} provides background and discuss related work. 
\autoref{sec:experiment} describes our experimental approach and dataset preparation. 
\autoref{sec:results} presents the results of our experimental investigation. 
\autoref{sec:conclusions} concludes.



% To this end, in this paper we adopt the recently presented GPT-2 natural language model to investigate its capabilities for translating natural language.
% GPT-2 recently made headlines~\cite{hern2019new} by the authors' announcement that they would not release their models until the public ramifications of doing so had been suitably discussed. 
% Their concerns stemmed from the model's impressive text generation capabilities in potential malicious applications (e.g., in mass generation of ``fake news'').
% However, as the models have since been released, they now present an attractive option for natural language processing, especially since they have demonstrated inherent multi-task capabilities---i.e. parsing and ``translation'' of text in a single step.
% In addition, GPT-2, like other transformer architectures, allows for \emph{transfer learning}, where the existing models, pretrained over the large unstructured WebText dataset, can be \emph{fine-tuned} to new domains.

% As such, in this paper we set out to use GPT-2 to translate natural language specifications in English into the Verilog \ac{HDL} by fine-tuning it over our own custom dataset of Task/Result pairs (which are somewhat akin to novice-level "textbook" problems (i.e., similar to those found in Vahid's classic textbook~\cite{vahid_digital_2010}).
% An example of such a Task/Result pair is presented in \autoref{fig:example}.




%\noteWarning{There has been much work so far on synthesizing code from natural language}

%\begin{itemize}
%    \item It is difficult to write computer programs in general
%    \item It is especially difficult to write HDLs (citation needed?)
%    \item Often descriptions of hardware can be much more concise and understandable than their code
%    \item Even when designers understand what they want to implement, producing the code output can be a challenge.
%    \item Simplifying the HDLs into more managable and modern languages (eg. Chisel, spinalHDL) is one way to address this,
%    \item But a different direction would allow for code to be written in a natural language.
%\end{itemize}

%\begin{itemize}
 %
%\item In this work, we want to explore the feasibility/effectiveness/potential for using natural language (GPT2) state of the art to convert natural language into Verilog.
%    \item We choose GPT2 because of reasons, including marketing hype / forbidden power / too scary to release.
 %   \begin{itemize}
 %       \item But more to do with the idea of multi-task learning; i.e., parsing and "translation" in a single step
 %   \end{itemize}
%\end{itemize}



% The rest of the paper is organised as follows...

%Overcoming the complexity of with the complexity innate in computer programming languages 


%\textit{Harnessing the forbidden power of GPT...might be worth giving some lip service to the "too scary to release" claims of OpenAI. Also, what sort of tone do we want to strike? Do we want to lean into the quirkiness of trying to train a second-year digital design student?}