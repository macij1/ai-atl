\subsection{Variations to the Existing Architecture}
The current architecture proposed by \cite{sabour2017dynamic} for the MNIST dataset is a shallow architecture with 3 layers and 3 additional reconstruction layers. The first layer is a convolutional layer consisting 256, 9$\times$9 kernels with a stride of 1 and ReLU activation. The second layer is a convolutional capsule layer with 32 channels of 8-dimensional convolutional capsules, with each primary capsule containing 8 convolutional units with a 9$\times$9 kernel and a stride of 2. The third layer has a 16-dimensional capsule per class, resulting in 10 capsules for the MNIST dataset. 

The first two reconstruction layers are fully connected layers with ReLU activation and 512 and 1024 units respectively, whereas the third layer is also a fully connected layer with Sigmoid activation and 784 units for the MNIST dataset. The sum of squared differences between the outputs of the logistic units and the input pixel intensities is taken as the loss function and it is minimized during the training phase.

We intend to use the classifier of the existing architecture to our network, subjected to one minor change. Instead of using $9\times9$ kernels in the first convolutional layer and the subsequent capsule layer, we use $3\times3$ kernels, in order to capture the lower level features with the smallest localized area.
As for the reconstruction network shown in figure \ref{fig:capsnet}, we use an architecture with five deconvolution layers \cite{decon2010} (\textcolor{red}{explain deconvolution -hj}) and a fully connected layer. The first layer is the fully connected layer with ReLU activation and 6272 units. Then the output of the fully connected layer is reshaped in to $7\times7\times128$ in order to feed to the first deconvolution layer. This layer consists of 128 number of $3\times3$ kernels with $(1,1)$ sub sampling. Second deconvolution layer consists of 64 number of $3\times3$ kernels with $(2,2)$ sub sampling. Third deconvolution layer consists of 32 number of $3\times3$ kernels with $(2,2)$ sub sampling. Final convolution layer consists of one $3\times3$ kernel with $(1,1)$ sub sampling, resulting $28\times28\times1$ image for the EMNIST data set.
 We consider this architecture, as illustrated by figure \ref{fig:base}, to be the base model throughout this paper. 
 
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{figures/capsnet.png}
  \caption{\textbf{TextCap Model}: Base model for the character classification. This model surpass the previous results on handwritten character recognition. }

 \label{fig:base}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{figures/capsnet.png}
  \caption{\textbf{TextCap Decoder}: Decoder network for the character reconstruction. Input to this network is obtained by masking the DigitCaps layer of the TextCap classifier}

 \label{fig:decoder}
\end{figure}

\cite{sabour2017dynamic} already established that model in figure \ref{fig:capsnet} achieves state-of-the-art performance for the full MNIST dataset, with 6000 data points per training class. We evaluate the base model for its performance when the number of data points available is as low as 200 data points per training class.

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.2]{figures/capsnet.png}
  \caption{\textbf{Original Model}: This is the original CapsNet model present in the original paper \cite{sabour2017dynamic}. }

 \label{fig:capsnet}
\end{figure}

