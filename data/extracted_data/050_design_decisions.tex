\section{Usability centric design}

\subsection{Deep learning models are just Python programs}
In a surprisingly short amount of time, machine learning grew from recognizing individual digits \cite{mnist} into autonomously playing StarCraft \cite{starcraft2}. Consequently, the neural networks themselves evolved rapidly from simple sequences of feed forward layers into incredibly varied numerical programs often composed of many loops and recursive functions.
To support this growing complexity, PyTorch foregoes the potential benefits of a graph-metaprogramming based approach to preserve the imperative programming model of Python. This design was pioneered for model authoring by Chainer\cite{Chainer} and Dynet\cite{DyNet}. PyTorch extends this to all aspects of deep learning workflows. Defining layers, composing models, loading data, running optimizers, and parallelizing the training process are all expressed using the familiar concepts developed for general purpose programming.


This solution ensures that any new potential neural network architecture can be easily implemented with PyTorch. For instance, layers (which in modern machine learning should really be understood as stateful functions with implicit parameters) are typically expressed as Python classes whose constructors create and initialize their parameters, and whose forward methods process an input activation. Similarly, models are usually represented as classes that compose individual layers, but let us state again that nothing forces the user to structure their code in that way. Listing \ref{lst:code_example} demonstrates how an entire model can be created by composing functionality provided by PyTorch such as 2d convolution, matrix multiplication, dropout, and softmax to classify gray-scale images. Note that linear layers are of course part of the library, but we show an example implementation to highlight how simple it is.

\begin{minipage}{\textwidth}
\begin{parcolumns}{2}
\colchunk{
\begin{minted}[fontsize=\small]{python}
class LinearLayer(Module):
   def __init__(self, in_sz, out_sz):
      super().__init__()
      t1 = torch.randn(in_sz, out_sz)
      self.w = nn.Parameter(t1)
      t2 = torch.randn(out_sz)
      self.b = nn.Parameter(t2)

   def forward(self, activations):
      t = torch.mm(activations, self.w)
      return t + self.b
\end{minted}
}
\colchunk{
\begin{minted}[fontsize=\small]{python}
class FullBasicModel(nn.Module):
   def __init__(self):
      super().__init__()
      self.conv = nn.Conv2d(1, 128, 3)
      self.fc = LinearLayer(128, 10)

   def forward(self, x):
      t1 = self.conv(x)
      t2 = nn.functional.relu(t1)
      t3 = self.fc(t1)
      return nn.functional.softmax(t3)
\end{minted}
}
\end{parcolumns}
\captionof{listing}{A custom layer used as a building block for a simple but complete neural network.}
\label{lst:code_example}
\end{minipage}
%\smallskip
%\medskip
\bigskip

This ``everything is a just a program'' philosophy is not limited to just the models, and applies to optimizers and data loaders as well. This facilitates the experimentation of new training techniques. For example, to implement the very popular generative adversarial networks, one needs to specify two separate models (the generator and the discriminator), and two loss functions that depend on both models at the same time. Rigid APIs would struggle with this setup, but the simple design employed in PyTorch easily adapts to this setting as shown in Listing~\ref{lst:gan}.

\begin{figure}[thp]
\centering 


\begin{minipage}{.6\textwidth}
\begin{minted}[fontsize=\small]{python}
discriminator = create_discriminator()
generator = create_generator()
optimD = optim.Adam(discriminator.parameters())
optimG = optim.Adam(generator.parameters())

def step(real_sample):
  # (1) Update Discriminator
  errD_real = loss(discriminator(real_sample), real_label)
  errD_real.backward()
  fake = generator(get_noise())
  errD_fake = loss(discriminator(fake.detach(), fake_label)
  errD_fake.backward()
  optimD.step()
  # (2) Update Generator
  errG = loss(discriminator(fake), real_label)
  errG.backward()
  optimG.step()

\end{minted}
\captionof{listing}{Simplified training of a generative adversarial networks.}
\label{lst:gan}
\end{minipage}
\end{figure}

Since PyTorch programs execute eagerly, all the features of Python are available throughout the whole design process. Print statements, standard debuggers, and common visualization tools like matplotlib all work as expected. Users do not have to wait for lengthy compilation before they can start running their programs, and more importantly intermediate computations can be observed to understand how a model works and whether its results are correct.

\subsection{Interoperability and extensibility}

Easy and efficient interoperability is one of the top priorities for PyTorch because it opens the possibility to leverage the rich ecosystem of Python libraries as part of user programs. Hence, PyTorch allows for bidirectional exchange of data with external libraries.
For example, it provides a mechanism to convert between NumPy arrays and PyTorch tensors using the \lstinline{torch.from_numpy()} function and \lstinline{.numpy()} tensor method. Similar functionality is also available to exchange data stored using the DLPack~\cite{dlpack} format.
Note that this exchange happens in both cases without any data copying -- objects on both sides only describe how to interpret a memory region which is shared among them.
Hence, those operations are actually extremely cheap, and take constant time no matter how large the converted arrays are.

Moreover, many of the critical systems are designed specifically to be extensible.
For instance, the automatic differentiation system allows users to add support for custom differentiable functions. To do that users can define a new subclass of \lstinline{torch.autograd.Function} that implements \lstinline{forward()} and \lstinline{backward()} methods, which specify the function and its derivative (or more formally the vector-Jacobian product).
Similarly new datasets can be added by subclassing \lstinline|torch.utils.data.Dataset|
and implementing two methods: \lstinline{__getitem__} (the indexing operator) and \lstinline{__len__} (the length operator), making datasets behave like (possibly lazy) lists. How these work is completely up to the implementer, and many users leverage other Python packages for data loading.
The \lstinline{DataLoader} class consumes objects conforming to this interface and provides an iterator over the data which takes care of shuffling, batching, parallelization, and management of pinned CUDA memory to improve throughput.

Most importantly, users are free to replace any component of PyTorch that does not meet the needs or performance requirements of their project. They are all designed to be completely interchangeable, and PyTorch  takes great care not to impose any particular solution.


\subsection{Automatic differentiation}

Since gradient based optimization is vital to deep learning, PyTorch must be able to automatically compute gradients of models specified by our users, and those can be arbitrary Python programs.
However, Python is a dynamic programming language that allows changing most behaviors at runtime, making ahead of time source-to-source differentiation cumbersome.
Instead, PyTorch uses the operator overloading approach, which builds up a representation of the computed function every time it is executed.
In its current implementation \cite{pytorch_autodiff}, PyTorch performs reverse-mode automatic differentiation, which computes the gradient of a scalar output with respect to a multivariate input. Differentiating functions with more outputs than inputs is more efficiently executed using forward-mode automatic differentiation, but this use case is less common for machine learning applications. PyTorch can be easily extended to perform forward-mode differentiation using array-level dual numbers \cite{Piponi-dual-numbers,Leuck-dual-numbers}.

Another interesting and uncommon feature of our system is that it can differentiate through code employing mutation on tensors, which is one of the basic building blocks of imperative programs.
To ensure safety, we have implemented a versioning system for tensors, which lets us track their modifications and ensure that we always use the data we expect.
One interesting tradeoff is that while we could utilize techniques like copy-on-write to support arbitrary programs, we chose to not go down this path, as performance-wise it is usually beneficial for the users to rewrite their code to ensure that no copies have to be performed.
Hence, while most mutations are benign and can be handled automatically, the really complicated cases result in a user error, which lets them know that they likely want to restructure the program.
This allows us to avoid introducing subtle and hard-to-find performance cliffs.

