\subsubsection{Speech Generation}
\label{sec:data:tts}
The speech generation datasets mainly consist of those for training the text normalization (TN) model and the prosody model (PM).  Both training data are augmented with an additional input feature of the Llama 3 embeddings to provide contextual information.

\textbf{Text normalization data.} Our TN training dataset includes 55K samples that cover a wide range of semiotic classes (\emph{e.g.}, number, date, time) that require non-trivial normalization. Each sample is a pair of written-form text and the corresponding normalized spoken-form text, with an inferred sequence of handcrafted TN rules that carry out the normalization.

\textbf{Prosody model data.} The PM training data includes linguistic and prosodic features extracted from a 50K-hour TTS dataset, which are paired transcripts and audios recorded by professional voice actors in studio settings.

\textbf{Llama 3 embedding.}
The Llama 3 embeddings are taken as the output of the 16th decoder layer. We work exclusively with the Llama 3 8B model and extract the embeddings for a given text (\emph{i.e.} written-form input text for TN or the audio transcript for PM) as if they are generated by the Llama 3 model with an empty user prompt. In a given sample, each chunk in the Llama 3 token sequence is explicitly aligned with the corresponding chunks in native input sequence for TN or PM, \emph{i.e.}, TN-specific text tokens (demarcated by unicode category) or phone-rate features respectively. This allows for training the TN and PM modules with streaming input of Llama 3 tokens and embeddings.
