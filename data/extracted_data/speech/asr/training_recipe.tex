\subsubsection{Speech Understanding}

Training of the speech module is done in two stages.
The first stage, speech pre-training, leverages unlabeled data to train a speech encoder that exhibits strong generalization capabilities across languages and acoustic conditions.
In the second stage, supervised fine-tuning, the adapter and pre-trained encoder are integrated with the language model, and trained jointly with it while the LLM stays frozen. This enables the model to respond to speech input.
This stage uses labeled data corresponding to speech understanding abilities.

Multilingual ASR and AST modeling often results in language confusion/interference, which leads to degraded performance. A popular way to mitigate this is to incorporate language identification (LID) information, both on the source and target side. This can lead to improved performance in the predetermined set of directions, but it does come with potential loss of generality. For instance, if a translation system expects LID on both source and target side, then the model will not likely to show good zero-shot performance in directions that were not seen in training.
So our challenge is to design a system that allows LID information to some extent, but keeps the model general enough such that we can have the model do speech translation in unseen directions.
To address this, we design system prompts which only contain LID for the text to be emitted (target side). There is no LID information for the speech input (source side) in these prompts, which also potentially allows it to work with code-switched speech.
For ASR, we use the following system prompt: {\tt Repeat after me in \{language\}:},
where {\tt \{language\}} comes from one of the 34 languages (English, French, \emph{etc.})
For speech translation, the system prompt is: {\tt Translate the following sentence into \{language\}:}.
This design has been shown to be effective in prompting the language model to respond in the desired language.
We used the same system prompts during training and inference.

\textbf{Speech pre-training.}
We use the self-supervised BEST-RQ algorithm \citep{chiu2022self} to pre-train the speech encoder.
We apply a mask of 32-frame length with a probability of 2.5\% to the input mel-spectrogram.
If the speech utterances are longer than 60 seconds, we perform a random crop of 6K frames, corresponding to 60 seconds of speech.
We quantize mel-spectrogram features by stacking 4 consecutive frames, projecting the 320-dimensional vectors to a 16-dimensional space, and performing a nearest-neighbor search with respect to cosine similarity metric within a codebook of 8,192 vectors.
To stabilize pre-training, we employ 16 different codebooks.
The projection matrix and codebooks are randomly initialized and are not updated throughout the model training.
The multi-softmax loss is used only on masked frames for efficiency reasons.
The encoder is trained for 500K steps with a global batch size of 2,048 utterances.

\textbf{Supervised finetuning.}
Both the pre-trained speech encoder and the randomly initialized adapter are further jointly optimized with \llamathree in the supervised finetuning stage. The language model remains unchanged during this process.
The training data is a mixture of ASR, AST, and spoken dialogue data.
The speech model for Llama 3 8B is trained for 650K updates, using a global batch size of 512 utterances and an initial learning rate of $10^{-4}$.
The speech model for Llama 3 70B is trained for 600K updates, using a global batch size of 768 utterances and an initial learning rate of $4\times10^{-5}$.
