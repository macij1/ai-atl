\subsubsection{Speech Understanding}

The training data can be categorized into two types.
The pre-training data includes a large amount of unlabeled speech, which is used to initialize the speech encoder in a self-supervised manner.
The supervised finetuning data includes speech recognition, speech translation, and spoken dialogue data; this data is used to unlock specific abilities when integrated with the large language model.

\textbf{Pre-training data.}
To pre-train the speech encoder, we curate a dataset of approximately 15M hours of speech recordings encompassing a large number of languages.
We filter our audio data using a voice activity detection (VAD) model and select audio samples with a VAD threshold above 0.7 for pre-training.
In speech pre-training data, we also focus on ensuring the absence of PII. We use the Presidio Analyzer to identify such PII. %

\textbf{Speech recognition and translation data.}
Our ASR training data contains 230K hours of manually transcribed speech recordings that span 34 languages.
Our AST training data contains 90K hours of translations in two directions: from 33 languages to English and from English to 33 languages.
This data contains both supervised and synthetic data generated using the NLLB toolkit~\citep{nllb2022}.
The use of synthetic AST data enables us to increase model quality for low-resource languages.
The speech segments in our data have a maximum length of 60 seconds.


\textbf{Spoken dialogue data.}
To finetune the speech adapter for spoken dialogue, we synthetically generate responses for speech prompts by asking the language model to respond to transcriptions of those prompts~\citep{fathullah2024audiochatllama}.
We generate synthetic data this way using a subset of the ASR dataset with 60K hours of speech.
In addition, we generate 25K hours of synthetic data by running the Voicebox TTS system \citep{le2024voicebox} on subsets of the data used to finetune Llama 3.
We used several heuristics to select a subset of finetuning data that matches the distribution of speech.
These heuristics include focusing on relatively short prompts with a simple structure and without non-text symbols.

