\subsubsection{Speech Understanding}

On the input side, the speech module consists of two successive modules: a speech encoder and an adapter.
The output of the speech module is directly fed into the language model as token representation, enabling direct interaction between speech and text tokens.
Furthermore, we incorporate two new special tokens to enclose the sequence of speech representations.
The speech module differs substantially from the vision module (see Section~\ref{section:vision}), which feeds multi-modal information into the language model via cross-attention layers. 
By contrast, the speech module generates embeddings that can be seamlessly integrated with text tokens, enabling the speech interface to leverage all the capabilities of the Llama 3 language model.

\textbf{Speech encoder.}
Our speech encoder is a Conformer~\citep{gulati2020conformer} model with 1B parameters.
The input to the model consists of 80-dimensional mel-spectrogram features, which are first processed by a stride-4 stacking layer followed by a linear projection to reduce the frame length to 40 ms.
The resulting features are processed by an encoder with 24 Conformer layers.
Each Conformer layer has a latent dimension of 1536, and consists of two Macron-net style feed-forward networks with dimension 4096, a convolution module with kernel size 7, and a rotary attention module \citep{su2024roformer} with 24 attention heads.


\textbf{Speech adapter.}
The speech adapter contains about 100M parameters. It is composed of a convolution layer, a rotary Transformer layer, and a linear layer.
The convolution layer has a kernel size of 3 and a stride of 2, which is designed to  reduce the speech frame length to 80ms. This allows the model to provide more coarse-grained features to the language model.
The Transformer layer has a latent dimension of 3072 and a feed-forward network with a dimension of 4096 which further processes the information from speech with context after the convolutional downsampling.
Finally, the linear layer maps the output dimension to match that of the language-model embedding layer.
