\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{assets/llama3_architecture.pdf}
    \caption{\textbf{Illustration of the compositional approach to adding multimodal capabilities to Llama 3 that we study in this paper.} This approach leads to a multimodal model that is trained in five stages: \textbf{(1)} language model pre-training, \textbf{(2)} multi-modal encoder pre-training, \textbf{(3)} vision adapter training, \textbf{(4)} model finetuning, and \textbf{(5)} speech adapter training.}
    \label{sph:fig:multimodal_model_overview}
\end{figure}

\section{Vision Experiments}
\label{section:vision}

We perform a series of experiments in which we incorporate visual-recognition capabilities into \llamathree via a compositional approach that consists of two main stages.
First, we compose a pre-trained image encoder \citep{xu2023demystifying} and the pre-trained language model by introducing and training a set of cross-attention layers between the two models \citep{alayrac2022flamingo} on a large number of image-text pairs.
This leads to the model illustrated in Figure~\ref{sph:fig:multimodal_model_overview}.
Second, we introduce temporal aggregator layers and additional video cross-attention layers that operate on a large collection of video-text pairs to learn the model to recognize and process temporal information from videos.

A compositional approach to foundation model development has several advantages: \textbf{(1)} it enables us to parallelize the development of the vision and language modeling capabilities; \textbf{(2)} it circumvents complexities of joint pre-training on visual and language data that stem from tokenization of visual data, differences in background perplexities of tokens originating from different modalities, and contention between modalities; \textbf{(3)} it guarantees that model performance on text-only tasks is not affected by the introduction of visual-recognition capabilities, and \textbf{(4)} the cross-attention architecture ensures that we do not have to expend compute passing full-resolution images through the increasingly LLM backbones (specifically, the feed-forward networks in each transformer layer), making it more efficient during inference.
We note that our multimodal models are still under development and not yet ready for release.

Before presenting the results of our experiments in Section~\ref{section:results_image_recognition} and~\ref{section:results_video_recognition}, we describe the data we used to train visual recognition capabilities, the model architecture of the vision components, how we scale training of those components, and our pre-training and post-training recipes.

\input{vision/data.tex}
\input{vision/model_architecture.tex}
\input{vision/model_scaling.tex}
\input{vision/training_recipe.tex}
\input{vision/post_training.tex}
\input{results/image_recognition.tex}
\input{results/video_recognition.tex}
