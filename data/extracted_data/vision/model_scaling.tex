\subsection{Model Scaling}
\label{section:vision_model_scaling}

After the visual-recognition components are added to \llamathree, the model contains self-attention layers, cross-attention layers, and a ViT image encoder.
To train adapters for the smaller 8B and 70B parameter models, we found a combination of data and tensor parallelization is the most efficient.
Model or pipeline parallelism does not increase efficiency at these scales because the gathering of model parameters would dominate the computation.
We do, however, use pipeline parallelism (in addition to data and tensor parallelism) when training the adapter for the 405B parameter model.
Training at this scale introduces three new challenges in addition to those outlined in Section~\ref{section:pretraining_model_scaling}: model heterogeneity, data heterogeneity, and numerical instabilities.

\textbf{Model heterogeneity.} The model computation is heterogeneous because more computation is performed on some tokens than on others.
In particular, image tokens are processed by the image encoder and the cross-attention layers, whereas text tokens are only processed by the language backbone.
This heterogeneity leads to bottlenecks in the scheduling of pipeline parallelism.
We address this problem by ensuring each pipeline stage contains five layers: namely, four self-attention layers in the language backbone and a cross-attention layer.
(Recall that we introduce a cross-attention layer after every fourth self-attention layer.)
In addition, we replicate the image encoder on all pipeline stages.
Because we train on paired image-text data, this enables us to perform load balancing between the image and text parts of the computation.

\textbf{Data heterogeneity.} The data is heterogeneous because, on average, images have more tokens than the associated text: an image has 2,308 tokens, whereas the associated text contains an average of only 192 tokens.
As a result, the computation of cross-attention layers requires more time and memory than the computation of self-attention layers.
We address this problem by introducing sequence parallelization in the image encoder, so that each GPU processes roughly the same number of tokens.
Because the average text size is relatively short, we also use a substantially larger micro-batch size (8 instead of 1).

\textbf{Numerical instabilities.} After the image encoder is added to the model, we find that performing gradient accumulation in bf16 led to numerical instabilities.
The most likely explanation for this is that image tokens are introduced into the language backbone via \emph{all} cross-attention layers.
This implies that numerical deviations in the representation of an image token have an outsized impact on the overall computation because the errors are compounded.
We address this by performing gradient accumulation in FP32.
