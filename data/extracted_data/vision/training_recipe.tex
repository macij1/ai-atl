\subsection{Pre-training}
\label{section:vision_training_recipe}

\textbf{Image.}
We initialize from the pre-trained text model and vision encoder weights.
The vision encoder is unfrozen, while the text model weights are kept frozen as explained above.
First, we train the model using 6B image-text pairs where each image is resized to fit within four tiles of $336 \times 336$ pixels.
We use a global batch size of 16,384 and a cosine learning rate schedule with initial learning rate $10 \times 10^{-4}$ and a weight decay of $0.01$.
The initial learning rate was determined based on small-scale experiments.
However,  these findings did not generalize well to very long training schedules and dropped the learning rate a few times during training when the loss values became stagnant.
After the base pre-training, we increase the image resolution further and continue training the same weights on the annealing dataset.
The optimizer is re-initialized via warm-up to learning rate $2 \times 10^{-5}$ and again follows a cosine schedule.

\textbf{Video.}
For video pre-training, we start from the image pre-trained and annealed weights as described above.
We add the video aggregator and cross-attention layers as described in the architecture, initialized randomly. We freeze all the parameters in the model except the video-specific ones (the aggregator and video cross-attention), and train them on the video pre-training data.
We use the same training hyperparameters as the image annealing stage, with small differences in the learning rate.
We uniformly sample 16 frames from the full video, and represent each frame using four chunks, each of size of $448 \times 448$ pixels.
We use an aggregation factor of 16 in the video aggregator, hence obtaining one effective frame, which the text tokens cross-attend to.
We use a global batch size of 4,096, a sequence length of 190 tokens, and a learning rate of $10^{-4}$ during training.
