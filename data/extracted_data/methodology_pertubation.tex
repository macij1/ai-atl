\subsection{Proposed Technique for Image Data Generation Using Perturbation of Instantiation Parameters}
\label{sec:meth_perturb}


From the concept of instantiation parameters in capsule networks, we can represent any character using a 16 dimensional vector \cite{sabour2017dynamic}. With a pre-trained decoder network, we can successfully reconstruct the original image, by using only this instantiation parameter vector. The intuition behind our proposed perturbation algorithm is that by adding controlled random noise to the values of the instantiation vector, we can create new images, which are significantly different from the original images, effectively increasing the size of the training dataset. Fig. \ref{fig:inst_change} illustrates the variations of an image, when one particular instantiation parameter is changed thusly. 

\begin{figure}[!h]
 \vspace{-2mm}
  \centering
  \includegraphics[scale=0.2]{figures/inst_change.png}
  \caption{Variation in characters with the perturbation of instantiation parameters}
 \label{fig:inst_change}
 \vspace{-2mm}
\end{figure}

Similarly, each of the instantiation parameter is responsible for a certain property of the image, individually or collectively. Hence, we propose a novel technique of generating a new dataset, from a dataset with limited amount of training samples, as illustrated by Fig. \ref{fig:system} and Algorithm \ref{algo:1}.

%\begin{figure}[!h]
%  \centering
%  \includegraphics[scale=0.35]{figures/flow.png}
%  \caption{Above figure shows the pipeline we used to train capsule network with few datapoints. \jathu{expain more}  }
%  \label{fig:perturbation}
%\end{figure}

 \begin{figure*} [h]
 \centering
    \subfigure[Initial Training of $M_1$ \label{fig:sys_1}]
    {\includegraphics[scale=0.45,clip]{figures/sys_block_1_crop.pdf}}
    \hspace{1.1cm}
    \subfigure[Generating instantiation parameters and reconstructed images \label{fig:sys_2}]
    {\includegraphics[scale=0.45]{figures/sys_block_2_crop.pdf}}
    \subfigure[Proposed decoder re-training technique \label{fig:sys_3}]
    {\includegraphics[scale=0.45]{figures/sys_block_3_crop.pdf}}
    \hspace{1cm}
    \subfigure[New image data generation by the proposed technique \label{fig:sys_4}]
    {\includegraphics[scale=0.5]{figures/sys_block_4_crop.pdf}}
    \subfigure[Training model $M_2$ afresh with the new dataset\label{fig:sys_5}]
    {\includegraphics[scale=0.45]{figures/sys_block_5_crop.pdf}}

\caption{The overall methodology for improving the decoder performance}
\label{fig:system}
\vspace{-4mm}
\end{figure*}
% \begin{algorithm}[!h]
% \caption{Image Perturbation}\label{algo:perturb}
% \begin{algorithmic}[0]
% %-------------- Input & Output -----------------
%   \State \textbf{Input:}  $k^{th}$ instantiation parameter of the $j^{th}$ data point of the $i^{th}$ class \textbf{$x_{i,j,k}$}, $w^{th}$ instantiation parameter of the $i^{th}$ class \textbf{$\sigma_{i,w}$}, decoder\textunderscore network model.
%   \State \textbf{Output:} Perturbed $k^{th}$ instantiation parameter of the $j^{th}$ data point of the $i^{th}$ class \textbf{$\tilde{x}_{i,j,k}$}
  
% \State $\mathit{\tilde{\sigma}_{i,w'}} \gets \text{Sorted(}\mathit{\sigma_{i,w}, w \in [0,15]}\text{)}$
% \State $\mathit{w} \gets \mathit{index}, \text{corresponds to }\textit{w'}=0 $
% \State $\mathit{noise} \gets \mathit{\tfrac{0.002}{\sigma_{max}}\times U[0,1]}$

% \If {$\mathit{x_{i,j,index}} > \mathit{0}$}
% \State $\mathit{x_{i,j,k}} \gets \mathit{x_{i,j,index}} + \mathit{noise}$
% \Else
% \State $\mathit{x_{i,j,index}} \gets \mathit{x_{i,j,index}} - \mathit{noise}$
% \EndIf
% \State $\mathit{\tilde{x}_{i,j,k}} \gets \textit{\text{decoder\textunderscore network}}\mathit{(x_{i,j,k})}$
% \end{algorithmic}
% \end{algorithm}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
% \begin{algorithm}[!h]
% \caption{Image Augmentation using perturbation}\label{algo:perturb}
% \begin{algorithmic}[0]
% %-------------- Input & Output -----------------
%   \State \textbf{Inputs:}  

%   \State \textbf{Output:} Reconstructed images using perturbed instantiation parameter \textbf{$I_{i,\hat{j},recon}$}.
% \begin{enumerate}
% \item  Calculate class variance $\sigma_{i,k} = varience_j(\widehat{C}_{i,j,k})$.   
% \item  Get $\tilde{\sigma}_{i,k'}$ $\gets$ Sort($\sigma_{i,k}$ over $k$) descending.    
% \item Get $\hat{k}=k$ corresponding to $k' = a$.
% \State $\phi_{i,k} \gets \max_{j}(x_{i,j,k})$ and $\psi_{i,k} \gets \min_{j}(x_{i,j,k})$
% \State $ \tau_{i,k} \gets \frac{\phi_{i,k} - \psi_{i,k}}{2}$
% \State get $\tau_k \gets average_i(\tau_{i,k})$
% \end{enumerate}
% \end{algorithmic}
% \end{algorithm}
%First, we train the network that we proposed in section \ref{sec:meth_full}, $M_1$, with 200 training samples per class. We pose no constraints in choosing the training samples, hence in this study, we choose the first 200 training samples per class in the dataset. Subsequently, we consider the capsule network,$M_{1,caps}$, and the decoder network,$M_{1,dec}$, separately. We obtain the instantiation parameters $x_{i,j,index}$ as the output of the capsule network, which can be masked and passed as the input of the decoder network. 

First, as illustrated by Fig. \ref{fig:sys_1}, we train the network proposed in Section \ref{sec:meth_full}, $M_1$, with dataset, $D_{\mathrm{original}}$, containing 200 training samples per class and all testing samples. Without loss of generality, we choose the first 200 training samples in each class in the dataset. Subsequently, we consider the trained capsule network, $M_{1,\mathrm{caps}}$, and the trained decoder network, $M_{1,dec}$, separately. Next, as illustrates by Fig. \ref{fig:sys_2}, we obtain the instantiation parameters, $C_{D,\mathrm{original}}$, corresponding to the training images, $I_{D,\mathrm{original}}$, in $D_{\mathrm{original}}$ as the output of the capsule network $M_{1,\mathrm{caps}}$, which can be masked as $\widehat{C}_{D,\mathrm{original}}$ and passed as the input of the decoder network $M_{1,\mathrm{dec}}$. We can obtain the corresponding reconstructed images $I_{D,\mathrm{recon}}$ as the output of $M_{1,\mathrm{dec}}$. Fig. \ref{fig:poor_recon} shows several such reconstructed images.

\begin{figure}[!h]
\vspace{-2mm}
\centering
\subfigure{\includegraphics[scale=0.8]{figures/final_test/good_7_ori.png}}
\subfigure{\includegraphics[scale=0.8]{figures/final_ori/good_7_ori.png}} \hspace{0.5cm}
\subfigure{\includegraphics[scale=0.8]{figures/final_test/good_114_ori.png}}
\subfigure{\includegraphics[scale=0.8]{figures/final_ori/good_114_ori.png}}\hspace{0.5cm}
\subfigure{\includegraphics[scale=0.8]{figures/final_test/good_6_ori.png}}
\subfigure{\includegraphics[scale=0.8]{figures/final_ori/good_6_ori.png}}
\caption{Original and reconstructed Image pairs with Model $M_1$}
\label{fig:poor_recon}
\vspace{-3mm}
\end{figure}


From Fig. \ref{fig:poor_recon}, we clearly observe that training with such low number of training samples result in poor reconstruction performance. The subtle variations in the input characters are absent from the reconstructions, in addition to being blurred. Hence, we cannot directly apply the concept of perturbation and create new training samples from a such poorly trained model. First, we attempt to eliminate the blurriness in the decoder network output, by proposing the following technique, illustrated by Fig. \ref{fig:sys_3}. For each reconstructed image in $I_{D,\mathrm{recon}}$, we perform unsharp masking \cite{826787} with $radius=1$, $threshold=1$ and $unsharp$ $strength = 10$ $times$, which sharpens the reconstructed images. Then we combine the new sharpened image set $I_{D,\mathrm{recon,sharped}}$ with the initial $I_{D,\mathrm{recon}}$ set, in order to create a new target set for the decoder $M_{1,\mathrm{dec}}$. Subsequently, we re-train the decoder for 10 epochs with this new target set, in order to obtain an improved decoder $\widetilde{M}_{1,\mathrm{dec}}$ which provides sharper reconstructions than $M_{1,\mathrm{dec}}$.

After training, there can be training samples which are not properly learned, and hence are wrongly reconstructed. If these wrongly reconstructed samples are considered for perturbing and creating new samples, it may result in miss-classified samples in the newly generated training dataset. Therefore, prior to applying perturbation and creating new training samples, it is necessary to remove such training samples, after the model is trained. 

Subsequently, we perform new data generation by perturbation, as illustrated by Fig. \ref{fig:sys_4}. For non-zero instantiation parameters in $\widehat{C}_{D,\mathrm{original}}$, we add random controlled noise (Algorithm \ref{algo:1}). Here, we perturb only one instantiation parameter at a time to generate new samples to avoid distortions. Hence, a method of selection of which instantiation parameter to perturb is required. We observed that, for a given class, there exists a relationship between the variance of an instantiation parameter and the actual physical variations in the generated images. Higher the variance, we observe rapid variations and vice versa. Hence, for each instantiation parameter $k \in [0,15]$ in each class $m \in M$, we calculate the variance, $\sigma_{m,k}$, across all the training samples that belongs to $m$, and sort in the descending order. We have 16 choices for the value of $a$, with $a=0$ representing the instantiation parameter with the highest variance and $a=15$ representing that with lowest variance. For this study, we generate two datasets with $a=0$ and $a=1$.

For a given $a$, we calculate the noise value to add for each instantiation parameter considered. The adjusted value of the instantiation parameter should not exceeded the maximum value that it can take for a given class. Adding noise to instantiation parameters without any restrictions will lead to various distortions in the reconstructed images, as illustrated in Fig. \ref{fig:too_much} below. The \textbf{H} has visually changed classes to \textbf{A}, and \textbf{a} is visually unrecognizable anymore. Such distortions are detrimental for a data generation technique. Therefore, we propose a carefully designed control mechanism to avoid such distortions, without manual elimination by visual inspection. 

\begin{figure}[!h]
\vspace{-2mm}
    \centering
    \includegraphics[scale=0.11]{images/4.png}
    \includegraphics[scale=0.11]{images/3.png}
    \caption{Distortions caused by adding uncontrolled noise}
    \label{fig:too_much}
    \vspace{-3mm}
\end{figure}

Hence, for each instantiation parameter $k$ of the class $m$, we calculate the maximum noise that can be added, $\tau_{m,k}$. We further constrain the noise by $\tau_{k}$, which is the average maximum noise that can be added for the instantiation parameter $k$ across all the classes, even though $\tau_{m,k}$ allows for higher values. This is to prevent sudden high variations occurring after perturbations. Hence, the noise value added for any $k$ is capped at $\tau_{k}$. Subsequently, we obtain the new reconstructed images, $I_{D,\mathrm{perturbed}}$, which are significantly different from the original training images, $I_{D,\mathrm{original}}$, by passing the perturbed instantiation parameter tensor to the decoder $\widetilde{M}_{1,\mathrm{dec}}$.


\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{algorithm}\caption{Image data generation using perturbation} \label{algo:1}
\algorithmicrequire \ Instantiation parameters $\widehat{C}$, $a^{th}$ highest variance, Decoder Network model ($\widetilde{M}_{dec}$). \newline
\algorithmicensure \ Perturbed images $I_{\mathrm{perturbed}}$

\begin{algorithmic}[1]
\State  Calculate class variance $\sigma_{m,k} = \mathrm{var}_j(\widehat{C}_{m,j,k})$.   
\State  Get $\tilde{\sigma}_{m,k'}$ $\gets$ $sort_k$($\sigma_{m,k}$) descending.    
\State Get $\hat{k}=k$ corresponding to $k' = a$.
%\State $\phi_{m,k} \gets \max_{j}(\widehat{C}_{m,j,k})$ and $\psi_{m,k} \gets \min_{j}(\widehat{C}_{m,j,k})$
\State $ \tau_{m,k} \gets \frac{\max_{j}(\widehat{C}_{m,j,k}) - \min_{j}(\widehat{C}_{m,j,k})}{2}$
\State get $\tau_k \gets \mathrm{avg}_i(\tau_{m,k})$
\ForEach {$\hat{j} \in [j]$}
\If {$\widehat{C}_{m,\hat{j},\hat{k}} > 0$ }
\State $\widehat{C}_{m,\hat{j},\hat{k}} \gets  \widehat{C}_{m,\hat{j},\hat{k}} + \min(\tau_{m,\hat{k}}, \tau_{\hat{k}})$
\Else
\State $\widehat{C}_{m,\hat{j},\hat{k}} \gets  \widehat{C}_{m,\hat{j},\hat{k}} - \min(\tau_{m,\hat{k}}, \tau_{\hat{k}})$
\EndIf
\EndFor
\State $I_{\mathrm{perturbed}} \gets \widetilde{M}_{\mathrm{dec}}(\widehat{C})$ 
\end{algorithmic}
\end{algorithm}

Finally, we have two new sets of training samples generated with $a=0$ and $a=1$. We combine these two sets and obtain $50$ random samples per class (user's discretion), to formulate the new dataset $D_{\mathrm{perturbed}}$. We combine $D_{\mathrm{perturbed}}$ and $D_{\mathrm{original}}$, which will effectively increase the number of training samples, solving our target problem. Subsequently, as illustrated by Fig. \ref{fig:sys_5}, we train a new model, $M_2$ with the new $D_{\mathrm{original}}+D_{\mathrm{perturbed}}$ dataset, re-train the decoder with the proposed re-training technique and obtain the final model for character classification.