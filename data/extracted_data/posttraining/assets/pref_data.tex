\begin{table}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
   {
  \begin{tabular}{@{}l@{\hspace*{0mm}}rrrrr@{}}
    \toprule
      & \textbf{\% of} & \textbf{Avg. \# turns} & \textbf{Avg. \# tokens} & \textbf{Avg. \# tokens} & \textbf{Avg. \# tokens} \\
     \textbf{Dataset} & \textbf{comparisons} &\textbf{per dialog} & \textbf{per example} &\textbf{in prompt} & \textbf{in response} \\
    \midrule
    General English & 81.99\% & 4.1 & 1,000.4 & 36.4 & 271.2 \\
    Coding & 6.93\% & 3.2 & 1,621.0 & 113.8 & 462.9 \\
    Multilingual & 5.19\% & 1.8 & 1,299.4 & 77.1 & 420.9 \\ 
    Reasoning and tools & 5.89\% & 1.6 & 707.7 & 46.6 & 129.9 \\
    \midrule
    Total & 100\% & 3.8 & 1,041.6 & 44.5 & 284.0 \\
    \bottomrule
  \end{tabular}}
  \vspace{0.3cm}
  \caption{\textbf{Statistics of human preference data.} We list statistics of the internally collected human preference data used for \llamathree alignment. We ask annotators to perform multi-turn dialogues with the models and make comparisons among responses at each turn. In post-processing, we split each dialogue to multiple examples at a turn level. Each example consists of a prompt (including previous dialog if available) and a response (e.g., chosen or rejected response).}
  \label{tab:pref_data}
\end{table}
