\section{Related work}
\label{sec:related}

\subsection{Analysis techniques}


Many current neural models of language learn representations that capture useful 
information about the form and meaning of the linguistic input.
Such neural representations are typically extracted from activations of various layers 
of a deep neural architecture trained for a target task such as automatic speech recognition 
or language modeling. 

A variety of analysis techniques have been proposed in the academic
literature to analyze and interpret representations learned by deep
learning models of language as well as explain their decisions; see
\citet{belinkov2019analysis} and \citet{alishahi2019analyzing} for a
review.
%
Some of the proposed techniques aim to  explain the behavior of a
network by tracking the response of individual or groups of neurons to
an incoming trigger \cite[e.g.,][]{nagamine2015exploring,krug2018neuron}.
%
In contrast, a larger body of work is dedicated to determining what type of linguistic
information is encoded in the learned representations. This type of analysis is the 
focus of our paper.
Two commonly used approaches to analyzing representations are:
\begin{itemize}
\item {\bf Probing techniques, or diagnostic classifiers,} i.e.\ methods which use the activations
from different layers of a deep learning architecture as input to a
prediction model \cite[e.g.,][]{adi2016fine,alishahi-etal-2017-encoding,hupkes2018visualisation,conneau-etal-2018-cram};
\item {\bf Representational Similarity Analysis (RSA)} borrowed from neuroscience
\cite{kriegeskorte2008representational} and used to correlate similarity
structures of two different representation spaces 
\cite{bouchacourt-baroni-2018-agents,chrupala-alishahi-2019-correlating,abnar-etal-2019-blackbox,abdou-etal-2019-higher}.
\end{itemize}
%Although both approaches have appealing properties, they have not been systematically compared to see
%whether they make compatible predictions about the type of linguistic information encoded by studied
%representations, and to carefully examine their reliability and relative expressive power.
We use both techniques in our experiments to systematically compare their output.


\subsection{Analyzing random representations}

Research on the analysis of neural encodings of language has shown that in some cases, 
substantial information can be decoded from activation patterns of randomly initialized, untrained 
recurrent networks. It has been suggested that the dynamics of the network together with the characteristics
of the input signal can result in non-random activation patterns \citep{DBLP:journals/corr/abs-1809-10040}. 
%For example, \citet{chrupala-alishahi-2019-correlating} show that untrained versions of Infersent 
%\citep{conneau-etal-2017-supervised} and BERT \citep{devlin-etal-2019-bert} encode aspects of the structural properties 
%of input sentences.

Using activations generated by randomly initialized recurrent networks has a history in speech recognition and 
computer vision. Two better-known families of such techniques are called Echo State Networks (ESN) 
\citep{jaeger2001echo} and Liquid State Machines (LSM) \citep{maass2002real}. The general approach 
(also known as reservoir computing) is as follows: the input signal is passed through a randomly 
initialized network to generate a nonlinear response signal. This signal is then used as input to train a model 
to generate the desired output at a reduced cost.

%{\it Most of this paragraph will be removed: 
%Both ESNs and LSMs provide a framework for training an output function on activations of random recurrent 
%neural networks from time-varying input. The main idea is that the random recurrent networks perform a large 
%variety of nonlinear transformations on the input signal, which allows for training linear combinations needed 
%to perform a predictive task. Using an untrained RNN leads to reduced training cost.}

We also focus on representations from randomly initialized neural
models but do so in order to show how training a model changes the
information encoded in the representations according to our chosen
analysis methods.


\subsection{Neural representations of phonology}

Since the majority of neural models of language work with text rather than speech, the bulk of work on 
representation analysis has been focused on (written) word and sentence representations. However, a number
of studies analyze neural representations of phonology learned by models that receive a speech signal as their 
input. 


As an example of studies that track responses of neurons to controled
input, \citet{nagamine2015exploring} analyze local representations
acquired from a deep model of phoneme recognition and show that both
individual and groups of nodes in the trained network are selective to
various phonetic features, including manner of articulation, place of
articulation, and voicing.  \citet{krug2018neuron} use a similar
approach and suggest that phonemes are learned as an intermediate
representation for predicting graphemes, especially in very deep
layers.

Others predominantly use diagnostic classifiers for phoneme and
grapheme classification from neural representations of speech.  In one
of the their experiments \citet{alishahi-etal-2017-encoding} use
a linear classifier to predict phonemes from local activation patterns
of a grounded language learning model, where images and their spoken
descriptions are processed and mapped into a shared semantic
space. Their results show that the network encodes substantial
knowledge of phonology on all its layers, but most strongly on the
lower recurrent layers.

Similarly, \citet{belinkov2017analyzing} use diagnostic classifiers to
study the encoding of phonemes in an end-to-end ASR system with
convolutional and recurrent layers, by feeding local (frame-based)
representations to an MLP to predict a phoneme label.  They show that
phonological information is best represented in lowest input and
convolutional layers and to some extent in low-to-middle recurrent
layers.  \citet{belinkov2019analyzing} extend their previous work to
multiple languages (Arabic and English) and different datasets, and
show a consistent pattern across languages and datasets where both
phonemes and graphemes are encoded best in the middle recurrent
layers.

None of these studies report on phoneme classification from randomly
initialized versions of their target models, and none use global (i.e., utterance-level) 
representations in their analyses.



