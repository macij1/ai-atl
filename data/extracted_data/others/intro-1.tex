\section{Introduction}
Language modeling is a long-standing research topic, dating back to in the 1950s with Shannon's application of information theory to human language, where he measured how well simple n-gram language models predict or compress natural language text. 
Since then, statistical language modeling became fundamental to many natural language understanding and generation tasks, ranging from speech recognition, machine translation, to information retrieval \cite{jelinek1998statistical,manning1999foundations,manning2009introduction}.

The recent advances on transformer-based large language models (LLMs), pretrained on Web-scale text corpora, significantly extended the capabilities of language models (LLMs). For example, OpenAI's ChatGPT and GPT-4 can be used not only for natural language processing, but also as general task solvers to power. Microsoft's Co-Pilot systems, for instance, can follow human instructions of complex new tasks performing multi-step reasoning when needed. LLMs are thus becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (AGI).

As the field of LLMs is moving fast, with new findings, models and techniques being published in a matter of months or weeks \cite{zhao2023survey,zhou2023comprehensive,liu2023pre,dong2022survey,huang2022towards}, AI researchers and practitioners often find it challenging to figure out the best recipes to build LLM-powered AI systems for their tasks. This paper gives a timely survey of the recent advances on LLMs. We hope this survey will prove a valuable and accessible resource for students, researchers and developers.


Statistical language models (SLMs) view text as a sequence of words, and estimate the probability of text as the product of their word probabilities. The dominating form of SLMs are Markov chain models known as the n-gram models, which compute the probability of a word conditioned on its immediate proceeding $n-1$ words. Since word probabilities are estimated using word and n-gram counts collected from text corpora, the model needs to deal with data sparsity (i.e., assigning zero probabilities to unseen words or n-grams) by using \emph{smoothing} where some probability mass of the model is reserved for unseen n-grams \cite{chen1999empirical}. N-gram models are widely used in many NLP systems. However, these models are incomplete in that they cannot fully capture the diversity and variability of natural language due to data sparsity.  



Early neural language models (NLMs) \cite{bengio2000neural,mikolov2010recurrent,graves2013generating} deal with data sparsity by mapping words to low-dimensional continuous vectors (embedding vectors) and predict the next word based on the aggregation of the embedding vectors of its proceeding words using neural networks. The embedding vectors learned by NLMs define a hidden space where the semantic similarity between vectors can be readily computed as their distance. 
This opens the door to computing semantic similarity of any two inputs regardless their forms (e.g., queries vs. documents in Web search \cite{huang2013learning}, sentences in different languages in machine translation \cite{sutskever2014sequence}) or modalities (e.g., image and text in image captioning \cite{fang2015captions}). Early NLMs are task-specific models, in that they are trained on task-specific data and their learned hidden space is task-specific.


\subsection{Brief History of Language Models}
%[Xavier] This section, does not read very "historical" despite the title. I would stress the timeline of when the different LMs appeared. For example: "Statistical Language Models, that started in the 90s...". Also it feels we lack a definition for "Language Model".
Looking at the history of prominent works on language models: Shannon's work \cite{shannon1951prediction} laid out the foundation of mathematical and statistical frameworks for language understanding, however that was not using any neural based modeling. 
In \cite{rumelhart1985learning}, Rumelhart and Hinton presented the early works on distributed word representation.
Another prominent work was the work "finding structure in time" by Elman \cite{elman1990finding}, in which he presented one of the early works on recurrent neural language models.
Later in \cite{mahoney2000fast}, Mahoney proposed a simple neural net architecture, that achieved state of the art performance in data compression, and it seemed to be open source (PAQ project). Although the model was technically more similar to logistic regression, but this was still a great work showing the promise of this approach.

Although all previous works had significant impact in pushing this field forward, "Neural Probabilistic Language Model" \cite{bengio2000neural} by Bengio et al. is sought to be the first good-performing neural language model on a small dataset and with real data (note that Elman's work proposed previously was using synthetic data).
The work by Schwenk et al. \cite{schwenk2006continuous} presented the first applications of these frameworks for machine translation (MT), achieved good results in MT competitions (WMT). However this model  didn't really scale well, as its performance dropped with training corpus size, when compared to n-gram baseline. 

Then in \cite{mikolov2011strategies, rnnlm}, Mikolov released "RNNLM", an open source neural language modeling toolkit, which was one of the first models that actually scaled well with more data, using different tricks. 
This was one of the first neural network based frameworks (which was very surprising back in 2010) that could beat n-gram based models on large datasets (which at that time had hundreds of millions of words). 
This was the first neural based model to report improving performance with data set size (i.e. the bigger the training set, the bigger the gain from the model). This work led to beginning of deep learning revolution in NLP space. Mikolov trained models with billions of parameters on hundreds of millions of words at the time.
RNNLM was also the first model that showed the potential of neural language model for text generation.
\iffalse
Fig \ref{fig:rnnlm} shows the high-level idea of this framework.
\begin{figure}[h]
\begin{center}
    \includegraphics [scale=0.6] {img/rnnlm.png}
\end{center}
  \caption{Feedforward neural network 4-gram model (on the left) and Recurrent
neural network language model (on the right), proposed by Mikolov et al. Courtesy of \cite{mikolov2011strategies}.}
\label{fig:rnnlm}
\end{figure}
\fi

%There were a series of other prominent works later by Tomas Mikolov, Richard Socher, Ronan Collobert, Jason Weston that showed more promising results by using neural language modeling on a wider range of tasks after that
Some of the key factors contributing to the success of these early prominent frameworks includes: creating better and large benchmarks (first being modified PTB), good choice of hyper-parameters, and more computational power (GPUs) which enabled researchers to train bigger and bigger models. 
New architectures (LSTM, GRU, and then Transformers), and techniques (such as new normalization, and attentions) also had good impact on the overall success of these models.


% \subsection{More Recent Language Models and applications}
% [Xsvier] I think we should mention and cite ULMFit https://arxiv.org/abs/1801.06146
Pre-trained language models (PLMs), unlike early NLMs, are task-agnostic. This generality also extends to the learned hidden embedding space. The training and inference of PLMs follows the \emph{pre-training and fine-tuning} paradigm, where language models with recurrent neural networks \cite{peters2018deep} or transformers \cite{devlin2018bert,liu2019roberta,he2020deberta} are pre-trained on Web-scale unlabeled text corpora for general tasks such as word prediction, and then finetuned to specific tasks using small amounts of (labeled) task-specific data. Recent surveys on PLMs include \cite{zhou2023comprehensive, han2021pre,qiu2020pre}. 

\begin{figure*}
\begin{center}
    \includegraphics [scale=0.57] {img/LLMCap.pdf}
\end{center}
  \caption{LLM Capabilities.}
  \label{fig:llm_cap}
\end{figure*}

%[Xavier] I think we should mention Table III as a way to understand what does "large" mean in this context
Large language models (LLMs) mainly refer to transformer-based neural language models \footnote{Recently, several very promising non-transformer LLMs have been proposed, such as the LLMs based on structured state space models \cite{gu2022S4,gu2023mamba}. See Section \ref{sec:LLM_challenges} for more details.} that contain tens to hundreds of billions of parameters which are pre-trained on massive text data, such as PaLM \cite{chowdhery2022palm}, LLaMA \cite{touvron2023llama}, and GPT-4 \cite{gpt4}, as summarized in Table III.  
Compared to PLMs, LLMs are not only much larger in model size, but also exhibit stronger language understanding and generation abilities, and more importantly, emergent abilities that are not present in smaller-scale language models. As illustrated in Fig.~\ref{fig:llm_cap}, these emergent abilities include (1) in-context learning, where LLMs learn a new task from a small set of examples presented in the prompt at inference time, (2) instruction following, where LLMs, after instruction tuning, can follow the instructions for new types of tasks without using explicit examples, and (3) multi-step reasoning, where LLMs can solve a complex task by breaking down that task into intermediate reasoning steps as demonstrated in the chain-of-thought prompt \cite{Wei2022COT}. 
LLMs can also be augmented by using external knowledge and tools \cite{mialon2023augmented,peng2023check} so that they can effectively interact with users and environment \cite{yao2022react}, and continually improve itself using feedback data collected through interactions (e.g. via reinforcement learning with human feedback (RLHF)). We will look at how LLMs can be used and augmented effectively in section \ref{sec:LLM_used}.

Through advanced usage and augmentation techniques, LLMs can be deployed as so-called AI agents: artificial entities that sense their environment, make decisions, and take actions. Previous research has focused on developing agents for specific tasks and domains. The emergent abilities demonstrated by LLMs make it possible to build general-purpose AI agents based on LLMs. 
While LLMs are trained to produce responses in static settings, AI agents need to take actions to interact with dynamic environment. Therefore, LLM-based agents often need to augment LLMs to e.g., obtain updated information from external knowledge bases, verify whether a system action produces the expected result, and cope with when things do not go as expected, etc. We will discuss in detail LLM-based agents in Section \ref{sec:LLM_used}. 


\begin{figure*}
\begin{center}
    \includegraphics [scale=0.52] {img/structure_paper.pdf}
\end{center}
  \caption{The paper structure.}
\label{fig:paper_structure}
\end{figure*}


\subsection{Paper Structure}
In the rest of this paper,
Section \ref{sec:LLM_models} presents an overview of the current state of the art of LLMs, focusing on three LLM families: GPT, LLaMA and PaLM. We then review some of the other representative LLMs that don't belong to any of those three families.
Section \ref{sec:LLM_built} discusses how LLMs are built, starting from data cleaning and tokenization, to model pre-training, instruction tuning, and alignment.
Section \ref{sec:LLM_used} explains how LLMs are used, and augmented, covering prompt engineering, augmenting LLMs through external knowledge, using external tools, and LLM agents.
Sections \ref{sec:llm_datasets} and \ref{sec:llm_performance} review popular datasets and benchmarks for evaluating LLMs, and summarize the reported LLM evaluation results.
%Section \ref{sec:LLM_applications} reviews the applications of LLMs in several important domains.
Finally, we conclude the paper in Section \ref{sec:LLM_challenges} by summarizing the challenges and future research directions. 
We also provide an overview of some of the most popular tools and frameworks for LLM development, deployment, and improvement, in Appendix.



