\section{Inference}
\label{section:inference}
We investigate two main techniques to make inference with the Llama 3 405B model efficient: \textbf{(1)} pipeline parallelism and \textbf{(2)} FP8 quantization.
We have publicly released our implementation of FP8 quantization.

\input{inference/pipeline_parallelism.tex}
\input{inference/fp8.tex}
