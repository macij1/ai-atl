\section{Evaluation}   \label{evaluation}


In this section we compare the performance of PyTorch with several other commonly-used deep learning libraries, and find that it achieves competitive performance across a range of tasks. All experiments were performed on a workstation with two Intel Xeon E5-2698 v4 CPUs and one NVIDIA Quadro GP100 GPU.

\subsection{Asynchronous dataflow}
We start by quantifying the ability of PyTorch to asynchronously execute dataflow on GPU. We use the built-in profiler \cite{autograd_profiler} to instrument various benchmarks and record a timeline of the execution of a single training step.

Figure \ref{fig:async_execution} shows a representative timeline of execution for the first few operations of a ResNet-50 model. The host CPU which queues the work quickly outpaces the execution of the operators on the GPU. This allows PyTorch to achieve almost perfect device utilization. In this example, GPU execution takes around three times longer than CPU scheduling. The exact ratio depends on the relative performance of the host CPU and the GPU, as well as the number of elements in each tensor and the average arithmetic complexity of the floating point computations to be performed on the GPU.

\begin{center}
  \includegraphics[width=\textwidth]{async_kernel_launches.pdf}
  \captionof{figure}{A trace of the first few operators of Resnet-50. The top row depicts the execution of the control flow running on the host CPU. The gray areas are Python code executed by its interpreter. The colored areas correspond to the work done on the host CPU to queue various operators (convolution, batch normalization, and so on). The bottom row shows the corresponding execution of those operators on the GPU. The arrows pair the two events in time.\label{fig:async_execution}}
\end{center}


\subsection{Memory management}
We used the NVIDIA profiler to trace the execution of the CUDA runtime as well as the execution of the CUDA kernels launched during one training iteration of the ResNet-50 model. As shown in Figure~\ref{fig:resnet_annotated_traces}, the behavior of the first iteration differs significantly from that of subsequent ones. At first, calls to the CUDA memory management functions (\verb|cudaMalloc| and \verb|cudaFree|) slow down the execution quite dramatically by blocking the CPU thread for long periods of time, hence lowering the utilization of the GPU. This effect disappears in subsequent iterations as the PyTorch caching memory allocator starts reusing previously allocated regions.

\begin{center}
  \includegraphics[width=\textwidth]{resnet50_annotated_traces.pdf}
  \captionof{figure}{Annotated traces of the execution of ResNet-50 on GPU. \label{fig:resnet_annotated_traces}}
\end{center}


\subsection{Benchmarks}

Finally, we can get an overall sense of single-machine eager mode performance of PyTorch by comparing it to three popular graph-based deep learning frameworks (CNTK, MXNet and TensorFlow), a define-by-run framework (Chainer), and production oriented platform (PaddlePaddle). The Appendix details all the steps needed to reproduce our setup.

Our results are summarized in Table~\ref{detailed_perf_results}. On all the benchmarks, the performance of PyTorch is within 17\% of that of of the fastest framework. We attribute this result to the fact that these tools offload most of the computation to the same version of the cuDNN and cuBLAS libraries.
\begin{table}[ht!]
\small
\centering
\begin{tabular}{l|c c c c c c}
 \toprule
 \multirow{2}{4.6em}{Framework} & \multicolumn{6}{c}{\it Throughput (higher is better)} \\
  & AlexNet & VGG-19 & ResNet-50 & MobileNet & GNMTv2 & NCF \\
\midrule

Chainer & $778 \pm 15$ & N/A & $\textbf{219} \pm 1$ & N/A & N/A & N/A \\
 CNTK & $845 \pm{8} $ & $ 84 \pm{3} $ & $210 \pm{1}$ & N/A & N/A & N/A \\
 MXNet & $\textbf{1554} \pm 22$ & $113 \pm 1$ & $218 \pm 2$ & $444 \pm 2$ & N/A & N/A \\
 PaddlePaddle  & $933\pm{123}$ & $112 \pm{2}$ & $192 \pm{4}$ & $\textbf{557}\pm{24}$ & N/A & N/A \\
 TensorFlow & $1422 \pm 27$ & $66 \pm 2$ & $200 \pm 1$ & $216 \pm 15 $ & $9631 \pm 1.3\%$ & $4.8e6 \pm 2.9\%$\\
 PyTorch & $1547 \pm 316$ & $\textbf{119} \pm 1$ & $212 \pm 2$ & $463 \pm 17$ & $\textbf{15512} \pm 4.8\%$ & $\textbf{5.4e6} \pm 3.4\%$\\

\bottomrule

\end{tabular}

 \caption{
  Training speed for 6 models using 32bit floats. Throughput is measured in images per second for the  AlexNet, VGG-19, ResNet-50, and MobileNet models, in tokens per second for the GNMTv2 model, and in samples per second for the NCF model. The fastest speed for each model is shown in bold.
  }
  \label{detailed_perf_results}

\end{table}

\subsection{Adoption}
The validity of design decisions and their impact on ease-of-use is hard to measure. As a proxy, we tried to quantify how well the machine learning community received PyTorch by counting how often various machine learning tools (including Caffe, Chainer, CNTK, Keras, MXNet, PyTorch, TensorFlow, and Theano) are mentioned on arXiv e-Prints since the initial release of PyTorch in January 2017. In Figure \ref{fig:pytorch_references} we report the monthly number of mentions of the word "PyTorch" as a percentage of all mentions among these deep learning frameworks. We counted tools mentioned multiple times in a given paper only once, and made the search case insensitive to account for various spellings.

\begin{center}
  \includegraphics[width=\linewidth]{arxiv_mentions.pdf}
  \captionof{figure}{Among arXiv papers each month that mention common deep learning frameworks, percentage of them that mention PyTorch.
  \label{fig:pytorch_references}}
\end{center}
