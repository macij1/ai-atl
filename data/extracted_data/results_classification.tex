\newcolumntype{C}[1]{>{\centering\arraybackslash}m{#1}}

\subsection{Handwritten Character Classification} \label{sec:results_classification}

Table \ref{table:class1} compares our results to the state-of-the-art. We include the results that we obtained with the full training sets, as well as using only 200 training samples per class. In both instances, we have used the full testing sets shown in Table \ref{table:data}, to report the average accuracies. We use a combination of marginal loss and the reconstruction loss for training as proposed in \cite{sabour2017dynamic}, and further, the training procedure followed for every experiment in this paper is similar to \cite{sabour2017dynamic}. For each dataset, we use ensembling to improve our model accuracy, and to avoid over fitting. We use cyclic learning rates for each 30 epochs, giving us 3 ensemble models with 90 epochs \cite{smith2017cyclical}.


%\begin{table}[!h]
%\label{table:class1}
%\caption{Comparison of TextCaps with state-of-the-art results}
%\centering
%\small
%\begin{tabular}{|C{2.6cm}|C{1.4cm}|C{1.6cm}||C{2.6cm}|C{1.4cm}|C{1.6cm}|}
%\hline
%\multicolumn{3}{|c||}{\textbf{EMNIST-Letters}} & \multicolumn{3}{c|}{\textbf{EMNIST-Balanced}}\\
% \hline
% \multirow{2}{2.6cm}{Model} & \multicolumn{2}{p{2.5cm}||}{Number of training samples} & \multirow{2}{2.6cm}{Model} & \multicolumn{2}{p{2.5cm}|}{Number of training samples} \\
 %\cline{2-3} \cline{5-6} 
%Implementation & With full train set &  With 200 samp/class & Implementation & With full train set &  With 200 samp/class	\\ [0.5ex] 
%\hline
%\multicolumn{1}{|l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}} &  85.15\%  & -& \multicolumn{1}{l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}} & 78.02\% & - \\ 

%\multicolumn{1}{|l|}{Wiyatno\textit{et al.}\cite{DBLP:journals/corr/abs-1803-01900}}  & 91.27\% &  - &  \multicolumn{1}{l|}{Dufourq \textit{et al.} \cite{2017arXiv170909161D}}  & 88.3\% & -\\ 

%\hline \hline
%\textbf{TextCaps}  & \textbf{95.34\%} & \textbf{94.10\%} &  \textbf{TextCaps}  & \textbf{90.40\%} & 87.82\%\\ 
%\hline
%\end{tabular}
%\end{table}

\begin{table}[!h]
\caption{Comparison of TextCaps with state-of-the-art results, the mean and the standard deviation from 3 trials are shown}
\vspace{1mm}
\label{table:class1}
\centering
\footnotesize
\begin{tabular}{|C{2.5cm}|C{2.15cm}|C{2.35cm}|}
\hline
\multicolumn{3}{|c|}{\textbf{EMNIST-Letters}}\\
 \hline
%  \multirow{2}{2.8cm}{Model} & \multicolumn{2}{C{4cm}|}{Number of training samples} \\ 
% \cline{2-3} 
Implementation & With full train set &  With 200 samp/class\\ [0.5ex] 
\hline
 \multicolumn{1}{|l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}} &  85.15\%  & - \\ 
 \multicolumn{1}{|l|}{Wiyatno\textit{et al.}\cite{DBLP:journals/corr/abs-1803-01900}}  & 91.27\% &  - \\
\hline
%\hline
 \textbf{TextCaps} & \textbf{95.36 $\pm$ 0.30\%} & \textbf{92.79 $\pm$ 0.30\%} \\ 
\hline \hline
%\end{tabular}
%\end{table}



%\begin{table}[!h]
%\centering
%\footnotesize
%\begin{tabular}{|C{2.5cm}|C{2.15cm}|C{2.35cm}|}

\multicolumn{3}{|c|}{\textbf{EMNIST-Balanced}}\\
 \hline
%  \multirow{2}{2.8cm}{Model} & \multicolumn{2}{C{4cm}|}{Number of training samples} \\ 
% \cline{2-3} 
Implementation & With full train set &  With 200 samp/class\\ [0.5ex] 
\hline
 \multicolumn{1}{|l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}} & 78.02\% & - \\ 
 \multicolumn{1}{|l|}{Dufourq \textit{et al.} \cite{2017arXiv170909161D}}  & 88.3\% & -\\ 
\hline 
%\hline
 \textbf{TextCaps} & \textbf{90.46 $\pm$ 0.22\%} & 87.82 $\pm$ 0.25\% \\ 
\hline \hline
%\end{tabular}
%\end{table}
%\begin{table}[!h]
%\centering
%\begin{tabular}{|C{2.6cm}|C{1.4cm}|C{1.6cm}||C{2.6cm}|C{1.4cm}|C{1.6cm}|}
%\hline
%\multicolumn{3}{|c||}{\textbf{EMNIST-Digits}}& \multicolumn{3}{c|}{\textbf{MNIST}}\\
% \hline
%  \multirow{2}{2.6cm}{Model} & \multicolumn{2}{p{2.2cm}||}{Number of training samples} & \multirow{2}{2.6cm}{Model} & \multicolumn{2}{p{2.2cm}|}{Number of training samples}\\
 
 %\cline{2-3} \cline{5-6} 
 
%Implementation & With full train set &  With 200 samp/class & Implementation & With full train set &  With 200 samp/class	\\ [0.5ex] 

%\hline
%\multicolumn{1}{|l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}}  & 95.90\% & - & \multicolumn{1}{l|}{Sabour \textit{et al.} \cite{sabour2017dynamic}} & 99.75\% & -\\ 

%\multicolumn{1}{|l|}{Dufourq \textit{et al.} \cite{2017arXiv170909161D}}  & 99.3\% & - & \multicolumn{1}{l|}{Cire{\c s}an \textit{et al.} \cite{DBLP:journals/corr/abs-1202-2745}}  &  99.77\% & - \\ 

%& &  & \multicolumn{1}{l|}{Wan \textit{et al.} \cite{pmlr-v28-wan13}}  & \textbf{99.79\%} & -\\

%\hline \hline
% \textbf{TextCaps}  & \textbf{99.78}\% & 98.96\% & \textbf{TextCaps} & 99.66\% & 98.68\%
%\\ 
%\hline
%\end{tabular}
%\end{table}


%\begin{table}[!h]
%\centering
%\footnotesize
%\begin{tabular}{|C{2.5cm}|C{2.15cm}|C{2.35cm}|}
%\hline
\multicolumn{3}{|c|}{\textbf{EMNIST-Digits}}\\
 \hline
%  \multirow{2}{2.8cm}{Model} & \multicolumn{2}{C{4cm}|}{Number of training samples} \\ 
% \cline{2-3} 
Implementation & With full train set &  With 200 samp/class\\ [0.5ex] 
\hline
 \multicolumn{1}{|l|}{Cohen \textit{et al.} \cite{2017arXiv170205373C}}  & 95.90\% & -\\ 
 \multicolumn{1}{|l|}{Dufourq \textit{et al.} \cite{2017arXiv170909161D}}  & 99.3\% & -\\
\hline 
%\hline
 \textbf{TextCaps} & \textbf{99.79 $\pm$ 0.11\%} & 98.96 $\pm$ 0.22\% \\
\hline \hline
%\end{tabular}
%\end{table}

%\begin{table}[!h]
%\centering
%\footnotesize
%\begin{tabular}{|C{2.5cm}|C{2.15cm}|C{2.35cm}|}
%\hline
\multicolumn{3}{|c|}{\textbf{MNIST}}\\
\hline
%  \multirow{2}{2.8cm}{Model} & \multicolumn{2}{C{4cm}|}{Number of training samples} \\ 
% \cline{2-3} 
Implementation & With full train set &  With 200 samp/class\\ [0.5ex] 
\hline
\multicolumn{1}{|l|}{Sabour \textit{et al.} \cite{sabour2017dynamic}} & 99.75\% & - \\ 
\multicolumn{1}{|l|}{Cire{\c s}an \textit{et al.} \cite{DBLP:journals/corr/abs-1202-2745}}  &  99.77\% & - \\ 
\multicolumn{1}{|l|}{Wan \textit{et al.} \cite{pmlr-v28-wan13}}  & \textbf{99.79\%} & -\\
\hline 
%\hline
 \textbf{TextCaps} & 99.71 $\pm$ 0.18\% & 98.68 $\pm$ 0.30\% \\
\hline \hline
%\end{tabular}
%\end{table}

%\begin{table}[!h]
%\centering
%\footnotesize
%\begin{tabular}{|C{2.5cm}|C{2.15cm}|C{2.35cm}|}
%\hline
\multicolumn{3}{|c|}{\textbf{Fashion MNIST}}\\
\hline
%  \multirow{2}{2.8cm}{Model} & \multicolumn{2}{C{4cm}|}{Number of training samples} \\ 
% \cline{2-3} 
Implementation & With full train set &  With 200 samp/class\\ [0.5ex] 
\hline
 \multicolumn{1}{|l|}{Xiao \textit{et al.} \cite{DBLP:journals/corr/abs-1708-07747}} & 89.7\% & - \\ 
 \multicolumn{1}{|l|}{Bhatnagar \textit{et al.} \cite{8313740}} & 92.54\% & - \\
 \multicolumn{1}{|l|}{Zhong \textit{et al.} \cite{DBLP:journals/corr/abs-1708-04896}}  & \textbf{96.35\%}  & -\\
\hline 
%\hline
 \textbf{TextCaps}  & 93.71 $\pm$ 0.64\% & 85.36 $\pm$ 0.79\%\\ 
\hline
\end{tabular}
\vspace{-0.3cm}
\end{table}

First, we describe the results we obtained with full training sets and compare with the state-of-the-art. On EMNIST-letters, we significantly outperform the state-of-the-art Wiyatno \textit{et al.} \cite{DBLP:journals/corr/abs-1803-01900} by 4.09\%. An average accuracy of 90.46\% was achieved by our system for the EMNIST-balanced dataset, which outperforms the state-of-the-art Dufourq \textit{et al.} \cite{2017arXiv170909161D} by 2.16\%. For EMNIST-digits dataset, TextCaps was able to surpass the state-of-the-art achieved by Dufourq \textit{et al.} \cite{2017arXiv170909161D} by 0.49\%. For MNIST and Fashion-MNIST, our system produced sub-state-of-the-art accuracy. Yet, our results are on-par.

Subsequently, we describe and compare the results we obtained with only 200 training samples per class. On EMNIST-letters, we exceed the state-of-the-art results by 1.52\%. However for  EMNIST-balanced, EMNIST-digits, MNIST we were able to achieve the state-of-the-art results. Even though our system did not surpass the state-of-the-art performance, we highlight that we were able to achieve a near state-of-the-art performance using only 8-10\% of the training data.  

%Further our system fails to produce competitive results on Fashion-EMNIST data set. This due to the inability of the first convolutional layer in the classifier to capture the high level features of the input image. We hope to resolve this problem in future work by introducing customize networks with stacked convolution layers to learn the complex features of an image. 
%\begin{table}[h]
%\centering
%\begin{tabular}{|l|l||l|l|}
%\hline
%\multicolumn{2}{|c||}{MNIST}& \multicolumn{2}{c|}{Fashion MNIST}\\
% \hline
% Sabour \textit{et al.} \cite{sabour2017dynamic} & 99.75\%  &  Xiao \textit{et al.} \cite{DBLP:journals/corr/abs-1708-07747} & 89.7\%
%\\ 
%
%Cire{\c s}an \textit{et al.} \cite{2012arXiv1202.2745C} &  99.77\%  &  Dufourq \textit{et al.} \cite{2017arXiv170909161D} & 90.6\%
%\\ 
%
% Wan \textit{et al.} \cite{pmlr-v28-wan13} & 99.79\%  & Bhatnagar \textit{et al.} \cite{8313740} & 92.54\%
%\\ 
%
%\hline \hline
%\textbf{TextCaps} & \textbf{99.66}\%  &  \textbf{TextCaps} & \textbf{93.57}\%
%\\ 
%\hline
%\end{tabular}
%
%\end{table}
%
%Describe the results
%In order to test the capablity of TextCaps, we evaluate the model on the EMNIST-balanced data set. As summarized in \ref{table:class2}, TextCap was able to achieve the state-of-the-art accuracy with 200 data points and with the full data set, our model surpass the state-of-the-art accuracy by 3\%. 

%\begin{table}[h!]
%\caption{Test accuracy for EMNIST-Letters data set with different number of data points.}
%\label{table:class2}
%\centering
%\begin{tabular}{|| c | c | c ||} 
% \hline
% \multirow{2}{4em}{Model} & \multicolumn{2}{p{4cm}||}{Number of Data Points}\\
% \cline{2-3}
%	& 200 		& full\\ [0.5ex] 
% \hline\hline
%\textit{OPTIMUN} \cite{2017arXiv170205373C}									&        -  		 &   78.02\%$\pm$0.92\%	\\ 
%\textit{EDEN} 	 \cite{2017arXiv170909161D}							 		&       -            & 	88.3\%$\pm$0.8\%			\\
%\textit{TextCap} 											 				& 87.56\%			 & 	\textbf{91.3\%} \jathu{need to include one error} 			\\[1ex]
% \hline
%\end{tabular}
%\end{table}

%\begin{table}
%\parbox{.45\linewidth}{
%\label{table:class1}
%\caption{Test accuracy for EMNIST-Letters data set with different number of data points.}
%\centering
%\begin{tabular}{| c | c | c |} 
% \hline
% \multirow{2}{2em}{Model} & \multicolumn{2}{c|}{Number of data points}\\
% \cline{2-3}
% 
% 			 	& 200 		& full\\ [0.5ex] 
% \hline
% $Style memory$\cite{DBLP:journals/corr/abs-1803-01900} 		 		& -	 & 91.27\%  \\ 
% $TextCap$ 		 	& \textbf{91.72}\% 	&  \\ [1ex]
% \hline
%\end{tabular}
%
%}
%\hfill
%\parbox{.45\linewidth}{
%\label{table:class2}
%\caption{Test accuracy for EMNIST-Letters data set with different number of data points.}
%\centering
%\begin{tabular}{| c | c | c |} 
% \hline
% \multirow{2}{2em}{Model} & \multicolumn{2}{c|}{Number of data points}\\
% \cline{2-3}
% 			& 200  & full \\ [0.5ex] 
% \hline
% $OPTIMUN$\cite{2017arXiv170205373C} 		& -& 78.02\%$\pm$0.92\%  \\
% $EDEN$\cite{2017arXiv170909161D} 		& -  &88.3\%$\pm$0.8\% \\  
% $TextCap$ 		& 87.56\% 	& \textbf{91.3\%} \\ [1ex]
% \hline
%\end{tabular}
%}
%\end{table}