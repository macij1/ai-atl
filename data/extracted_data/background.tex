\section{Background and Related Work}
\label{background}

Current DL IRs, including NNVM's current
representation, are heavily inspired by dataflow programming and
related computation graph abstractions made popular by previous frameworks.

For example, TensorFlow \cite{tensorflow} is an iteration on previous work at Google, such
as DistBelief \cite{distbelief}. These frameworks have evolved out of dataflow
programming paradigms in which the abstractions are operators with input and
output connections. The semantics provided by these languages have been
sketched in previous work \cite{tf_comp_model}.

TensorFlow employs a dataflow graph of primitive operators extended with restricted control
edges to represent differentiable programs. This representation is sufficient
for many state-of-the-art models and provides an implementation of reverse mode
automatic differentiation~{\cite{ad_survey, tensorflow}}.
%
TensorFlow can be viewed as a deeply embedded DSL (eDSL), where the result of executing
user's Python script is a computation graph which can then be optimized and
transformed before execution.
%
Furthermore, because the graph only exposes high-level nodes, it is possible
for the program to be portable to heterogeneous devices, and executing a sub-graph
on a given device requires implementation of only those operators for the device.
 Unfortunately, this programming model has limitations.
%
Because the topology is fixed before execution, TensorFlow does not lend itself
well to certain applications. As an example, unmodified TensorFlow does not support
building models where the shape of the computation graph is dependent on the input.
While there does exist a library to mitigate this particular problem
(see \cite{tensorflowfold}), this pattern suggests that should new dependencies become
of interest in the future, similar libraries would also have to be written to address each one,
entailing considerable engineering effort.

Dynamic frameworks such as Chainer \cite{chainer_learningsys2015},
PyTorch \cite{pytorch_ad}, Gluon, and TensorFlow eager-mode \cite{tf_eager}
alleviate this problem by moving from the define-then-run model
to the define-by-run model. PyTorch embeds primitives in Python that construct
dynamic dataflow graphs. Control flow is executed in the Python interpreter and
the dataflow is executed by the framework code with is implemented as Python extension.
However when using dynamic frameworks information about control flow is lost, reducing
the ability to optimize them. Additionally, dynamic frameworks need to re-optimize any time
the graph topology changes, costing CPU cycles and the overhead of moving data between
the host and accelerators. This can be solved by transforming the Python code but is effectively
the same as a static framework where Python is the input IR.

Previous work in higher-order differentiation is relevant and has informed
the Relay design. In particular we have drawn inspiration from
various implementations of automatic differentiation \cite{beautiful_diff, ad_survey, haskell_ad, toplas_reverse, wang_reverse, DLS, DDF}.
In particular we are interested in techniques that can compute higher order gradients of higher order programs.

Our work is part of the \tvm stack \cite{TVMSysML}, which is focused on compiling efficient
kernel implementations for deep learning frameworks such as MxNet.

Recent research on the \tvm stack \cite{TVMSysML} has been focused on producing efficient
operators (i.e., dense linear algebra kernels), such as generalized matrix multiplication (GEMM) or
convolutions. This line of research has focused on low-level performance, but demonstrated the need to
tune the high-level computation graph, the operators, and accelerators in tandem to achieve the best performance.
High-level transformations on the input program are especially important for the tensorization problem.
Tensorization is the analogous process to vectorization in existing compilers, and involves the optimizer
decomposing and matching programs to the underlying hardware tensor operations exposed.
This problem is more challenging due to being multi-dimensional, mixed size, and non-finite,
unlike the analogous SIMD primitives.

The \tvm stack is designed to enable a series of fundamental optimizations:

\begin{itemize}
    \item High-level optimizations, such as operator fusion and layout change
    \item Memory reuse at the graph and operators level
    \item Tensorized computations
    \item Latency hiding (traditional hardware provides this abstraction,
          but new accelerators push this burden to the compiler writers)
\end{itemize}

There are multiple related engineering efforts, the primary ones being from Google and Facebook.
Facebook has been building an efficient ML stack composed of many projects
including Tensor Comprehensions \cite{tensor_comprehensions} and Glow \cite{glow}.
Tensor Comprehensions are positioned in a similar space as \tvm, but employs
different techniques, such as using polyhedral compilation rather than algorithmic schedules.
The Glow compiler \cite{glow} is similar to NNVM and intended to
be a compiler for high-level computation graphs. Glow's design is closer to existing
computation graphs, does not appear to be a full language, and is less focused
on full-stack tuning.

TensorFlow's XLA is very similar to the complete TVM stack and is focused on providing
a lower-level intermediate representation for TensorFlow's computation graph. Relay is
designed to replace the user-visible graph with a higher-level abstraction and make it
possible for users to write frameworks like TensorFlow and PyTorch in pure Python.
