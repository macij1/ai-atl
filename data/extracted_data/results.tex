\section{Results}
\label{sec:results}

\begin{figure*}
  \centering
  \includegraphics[scale=0.65]{fig/trans-asr/plot.png}
  \caption{Results of diagnostic and RSA analytical methods applied to
    the Transformer-ASR model. The score is RER for the diagnostic methods and Pearson's $r$ for RSA.}
  \label{fig:results-trans-asr}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[scale=0.65]{fig/rnn-vgs/plot.png}                                                                 
  \caption{Results of diagnostic and RSA analytical methods applied to
    the RNN-VGS model. The score is RER for the diagnostic methods and Pearson's $r$ for RSA. }
  \label{fig:results-rnn-vgs}
\end{figure*}

\begin{figure*}
  \centering
  \includegraphics[scale=0.65]{fig/rnn-asr/plot.png}                                                                 
  \caption{Results of diagnostic and RSA analytical methods applied to
    the RNN-ASR model. The score is RER for the diagnostic methods and Pearson's $r$ for RSA. }
  \label{fig:results-rnn-asr}
\end{figure*}

Figures~\ref{fig:results-trans-asr}--\ref{fig:results-rnn-asr} display
the outcome of analyzing our target models. All three figures are
organized in a $2\times 3$ matrix of panels, with the top row showing
the diagnostic methods and the bottom row the RSA methods; the first
column corresponds to local scope; column two and three show global
scope with mean and attention pooling respectively.  The data points
are displayed in the order of the hierarchy of layers for each
architecture, starting with the input (layer id = 0).  In all the
reported experiments, the score of the diagnostic classifiers
corresponds to relative error reduction (RER), whereas for RSA we show
Pearson's correlation coefficient. For methods with trainable
parameters we show three separate runs with different random seeds in
order to illustrate the variability due to parameter initialization.

\begin{figure}[htb]
  \centering
  \includegraphics[scale=0.45]{fig/rnn-vgs/r2_partial.png}
  \caption{Results of global RSA with mean pooling on the RNN-VGS
    model, while controling for visual similarity. The score reported
    is the square root of the absolute value of the coefficient of
    partial determination $R^2_{\text{partial}}$.}
  \label{fig:rnn-vgs-r2_partial}
\end{figure}

Figure~\ref{fig:rnn-vgs-r2_partial} shows the results of global RSA
with mean pooling on the RNN-VGS target model, while
controling for visual similarity as a confound. 


We will discuss the patterns of results observed for each model separately in the following sections.

\subsection{Analysis of the Transformer-ASR model}
\label{sec:results-trans-asr}

As can be seen in Figure~\ref{fig:results-trans-asr}, most reported experiments (with the exception of 
the local RSA) suggest that phonemes are best encoded in pre-final layers of the deep network. 
The results also show a strong impact of learning on the predictions of the analytical methods, 
as is evident by the difference between the performance using representations of the trained 
versus randomly initialized models. 

Local RSA shows low correlation values overall, and does not separate the trained versus random conditions well. 

\subsection{Analysis of the RNN-VGS model}
\label{sec:results-rnn-vgs}

Most experimental findings displayed in
Figure~\ref{fig:results-rnn-vgs} suggest that phonemes are best
encoded in RNN layers 3 and 4 of the VGS model.
They also show that the representations extracted from the trained model encode
phonemes more strongly than the ones from the random version of the
model.

However, the impact of learning is more salient with 
global than local scope: the scores of both local classifier and local 
RSA on random vs.\ trained representations are close to each other for all layers. For the 
global representations the performance on trained representations quickly diverges 
from the random representations from the first RNN layer onward.

Furthermore, as demonstrated in Figure~\ref{fig:rnn-vgs-r2_partial},
for top RNN layers of this architecture, the correlation between
similarities in the neural activation space and the similarities in
the phoneme string space is not solely due to both being correlated to
visual similarities: indeed similarities in activation space contribute
substantially to predicting string similarities, over and above the
visual similarities.

\subsection{Analysis of the RNN-ASR model}
The overall qualitative patterns for this target model are the same as
for RNN-VGS. The absolute scores for the global diagnostic variants
are higher, and the curves steeper, which may reflect that the
objective for this target model is more closely aligned with encoding
phonemes than in the case of RNN-VGS.

\subsection{RNN vs Transformer models}
In the case of the local diagnostic setting there is a marked contrast
between the behavior of the RNN models on the one hand and the
Transformer model on the other: the encoding of phoneme information
for the randomly initialized RNN is substantially stronger in the
higher layers, while for the randomly initialized Transformer the
curve is flat. This difference is likely due to the very different
connectivity in these two architectures.

With random weights in
RNN layer $i$, the activations at time $t$ are a function of the
features from layer $i-1$ at time $t$, mixed with the features from
layer $i$ at time $t-1$. There are thus effects of depth that may make
it easier for a linear diagnostic classifier to classify phonemes from
the activations of a randomly initialized RNN: (i) features are
re-combined among themselves, and (ii) local context features are also
mixed into the activations.

The Transformer architecture, on the other hand, does not have the
local recurrent connectivity: at each timestep $t$ the activations are
a combination of all the other timesteps and already in the first
layer, so with random weights, the activations are close to random,
and the amount of information does not increase with layer depth.

In the global case, in the activations from random RNNs, pooling
across time has the effect of averaging out the vectors such that they
are around zero which makes them uninformative for the global
classifier: this does not happen to trained RNN
activations. Figure~\ref{fig:std-rnn-vgs} illustrates this point by
showing the standard deviations of vectors of mean-pooled activations
of each utterance processed by the RNN-VGS model for the randomly
initialized and trained conditions, for the recurrent
layers.\footnote{Only the RNN layers are show, as the different scale
  of activations in different layer types would otherwise obscure the
  pattern.}

\begin{figure}[htb]
  \centering
  \includegraphics[scale=0.45]{fig/rnn-vgs/pooled_feature_std.png}
  \caption{Standard deviation of pooled activations of the RNN layers for the RNN-VGS
    model.}
  \label{fig:std-rnn-vgs}
\end{figure}

\subsection{Summary of findings}
\label{sec:results-summary}

Here we discuss the impact of each factor in the outcome of our
analyses.
\paragraph{Choice of method.}
The choice of RSA versus diagnostic classifier interacts with scope,
and thus these are better considered as a combination. Specifically,
local RSA as implemented in this study shows only weak
correlations between neural activations and phoneme labels. It is
possibly related to the range restriction of point biserial
correlation with unbalanced binary variables. 

\paragraph{Impact of learning.}
Applied to the global representations, both analytical methods
are equally sensitive to learning. The results on random vs.\
trained representations for both methods start to diverge noticeably
from early recurrent layers. The separation for the local diagnostic
classifiers is weaker for the RNN models.


\paragraph{Representation scope.}
Although the temporal scale of the extracted representations
has not received much attention
and scrutiny, our experimental findings suggest that it is an
important choice. Specifically, global representations are more
sensitive to learning, and more consistent across different analysis
methods. Results with attention-based learned pooling are in general more
erratic than with mean pooling. This reflects the fact that analytical
models which incorporate learned pooling are more difficult to optimize
and require more careful tuning compared to mean pooling.

\subsection{Recommendations}
\label{sec:recommendations}
Given the above findings, we now offer tentative recommendations on
how to carry out representational analyses of neural models.
\begin{itemize}
\item Analyses on randomly initialized target models should be run as a baseline.
  Most scores on these models were substantially
  above zero: some relatively close to scores on trained models.
\item It is unwise to rely on a single analytical approach, even a
  widely used one such as the local diagnostic classifier. With solely this method
  we would have concluded that, in RNN models, learning has only a weak effect on the
  encoding of phonology.
\item Global methods applied to pooled representations should be
  considered as a complement to standard local diagnostic methods. In
  our experiments they show more consistent results.
\end{itemize}

