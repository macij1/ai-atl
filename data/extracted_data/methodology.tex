This sections outlines our approach. Prior to experimenting with reduced datasets, in Section \ref{sec:meth_full}, we attempt to surpass the state-of-the-art results for several hand written digit databases including EMNIST balanced, EMNIST letters and EMNIST digits, with the use of all the training samples provided. Subsequently, we attempt to achieve the state-of-the-art performance using a limited number of training samples, as low as 200 training samples per class, as opposed to, for example, 2400 data points per class in EMNIST balanced and 4800 data points per class in EMNIST letters.

In order to address the drawbacks faced when training the classifier with low number of training samples, we propose a novel technique for increasing the number of training samples in Section \ref{sec:meth_perturb}. We perform a comprehensive analysis of the effect of the loss function in reconstruction, in Section \ref{sec:meth_loss}.