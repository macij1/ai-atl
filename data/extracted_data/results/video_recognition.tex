\subsection{Video Recognition Results}
\label{section:results_video_recognition}

We evaluate our video adapter for Llama 3 on three benchmarks:
\begin{itemize}
\item \textbf{PerceptionTest}~\citep{patraucean2023perception} evaluates the model's ability to answer temporal reasoning questions focusing on skills (memory, abstraction, physics, semantics) and different types of reasoning (descriptive, explanatory, predictive, counterfactual). It consists of $11.6K$ test QA pairs, each with an on-average $23s$ long video, filmed by $100$ participants worldwide to show perceptually interesting tasks. We focus on the multiple-choice question answering task, where each question is paired with three possible options. We report performance on the held-out test split which is accessed by submitting our predictions to an online challenge server.\footnote{See \url{https://eval.ai/web/challenges/challenge-page/2091/overview}.}

\item \textbf{NExT-QA}~\citep{xiao2021next} is another temporal and causal reasoning benchmark, with a focus on open-ended question answering.
It consists of $1K$ test videos each on-average $44s$ in length, paired with $9K$ questions. The evaluation is performed by comparing the model's responses with the ground truth answer using Wu-Palmer Similarity (WUPS)~\citep{wu1994verb}.\footnote{See \url{https://github.com/doc-doc/NExT-OE}.}

\item \textbf{TVQA}~\citep{lei2018tvqa} evaluates the model's ability to perform compositional reasoning, requiring spatiotemporal localization of relevant moments, recognition of visual concepts, and joint reasoning with subtitle-based dialogue. This dataset, being derived from popular TV shows, additionally tests for the model's ability to leverage its outside-knowledge of those TV shows in answering the questions. It consists of over $15K$ validation QA pairs, with each corresponding video clip being on-average $76s$ in length. It also follows a multiple-choice format with five options for each question, and we report performance on the validation set following prior work~\citep{openai2023gpt4blog}.

\item \textbf{ActivityNet-QA}~\citep{yu2019activityqa} evaluates the model's ability to reason over long video clips to understand actions, spatial relations, temporal relations, counting, etc. It consists of $8K$ test QA pairs from $800$ videos, each on-average $3$ minutes long. For evaluation, we follow the protocol from prior work~\citep{gemini2023gemini,lin2023video,Maaz2023VideoChatGPT}, where the model generates short one-word or one-phrase answers, and the correctness of the output is evaluated using the GPT-3.5 API which compares it to the ground truth answer. We report the average accuracy as evaluated by the API.
\end{itemize}

When performing inference, we uniformly sample frames from the full video clip and pass those frames into the model with a short text prompt. Since most of our benchmarks involve answering multiple-choice questions, we use the following prompt: {\tt Select the correct answer from the following options: \{question\}.  Answer with the correct option letter and nothing else}. For benchmarks that require producing a short answer ({\em e.g.}, ActivityNet-QA and NExT-QA), we use the following prompt: {\tt Answer the question using a single word or phrase. \{question\}}. For NExT-QA, since the evaluation metric (WUPS) is sensitive to the length and the specific words used, we additionally prompt the model to be specific and respond with the most salient answer, for instance specifying ``living room'' instead of simply responding with ``house'' when asked a location question. For benchmarks that contain subtitles ({\em i.e.}, TVQA), we include the subtitles corresponding to the clip in the prompt during inference.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{\input{results/tables/video_recognition}}
    \caption{\textbf{Video understanding performance of our vision module attached to Llama 3.} We find that across range of tasks covering long-form and temporal video understanding, our vision adapters for \llama{3} 8B and 70B parameters are competitive and sometimes even outperform alternative models.}
    \label{table:video_recognition}
\end{table}

We present the performance of Llama 3 8B and 70B in Table~\ref{table:video_recognition}.
We compare Llama 3's performance with that of two Gemini and two GPT-4 models. Note that all our results are zero-shot, as we do not include any part of these benchmarks in our training or finetuning data. We find that our Llama 3 models that train a small video adapter during post-training are very competitive, and in some cases even better, than other models that potentially leverage native multimodal processing all the way from pre-training.
Llama 3 performs particularly well on video recognition given that we only evaluate the 8B and 70B parameter models.
Llama 3 achieves its best performance on PerceptionTest, suggesting the model has a strong ability to perform complex temporal reasoning.  On long-form activity understanding tasks like ActivityNet-QA, Llama 3 is able to obtain strong results even though it is processing only up to 64 frames, which means that for a 3-minute long video the model only processes one frame every 3 seconds.
