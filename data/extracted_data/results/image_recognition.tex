\subsection{Image Recognition Results}
\label{section:results_image_recognition}

We evaluate the performance of the image understanding capabilities of \llamathree on a range of tasks spanning natural image understanding, text understanding, charts understanding and multimodal reasoning:
\begin{itemize}
\item \textbf{MMMU}~\citep{yue2023mmmu} is a challenging dataset for mulitmodal reasoning where model is expected to understand
images and solve college-level problems spanning 30 different disciplines. This includes both multiple-choice and open ended
questions. We evaluate our model on the validation set with 900 images, in line with other works.

\item \textbf{VQAv2}~\citep{vqav2} tests the ability of a model to combine image understanding, language understanding and
commonsense knowlege to answer generic questions about natural images

\item \textbf{AI2 Diagram}~\citep{Kembhavi2016ADI} evaluates models capability to parse scientific diagrams
and answer questions about the same. We use the same evaluation protocol as Gemini and x.ai, and report scores using a transparent bounding box.

\item \textbf{ChartQA}~\citep{masry-etal-2022-chartqa} is a challenging benchmark for charts understanding. This requires
model to visually understand different kinds of charts and answer logical questions about the charts.

\item \textbf{TextVQA}~\citep{singh2019towards} is a popular benchmark dataset that requires
models to read and reason about text in images to answer questions about them. This tests the
OCR understanding ability of the model on natural images.

\item \textbf{DocVQA}~\citep{Mathew2020DocVQAAD} is a benchmark dataset focused on document analysis and recognition.
It contains images of a wide range of documents which evaluates a model's ability to perform OCR understanding
and reason about the contents of a document to answer questions about them.
\end{itemize}

Table~\ref{table:image_recognition} presents the results of our experiments.
The results in the table show that our vision module attached to \llamathree performs competitively across a wide range of image-recognition benchmarks at varying model capacities.
Using the resulting \llamathree-V 405B model, we outperform GPT-4V on all benchmarks, while being slightly behind Gemini 1.5 Pro and Claude 3.5 Sonnet.
\llamathree 405B appears particularly competitive on document understanding tasks.

\begin{table}[t]
    \centering
    \resizebox{\linewidth}{!}{\input{results/tables/image_recognition}}
    \caption{\textbf{Image understanding performance of our vision module attached to \llamathree.} We compare model performance to GPT-4V, GPT-4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet. $^{\triangle}$Results obtained using external OCR tools.}
    \label{table:image_recognition}
\end{table}
