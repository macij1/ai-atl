{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies.\n",
    "# !pip install asyncio==3.4.3 asyncpg==0.27.0 cloud-sql-python-connector[\"asyncpg\"]==1.2.3\n",
    "# !pip install numpy==1.22.4 pandas==1.5.3\n",
    "# !pip install pgvector==0.1.8\n",
    "# !pip install langchain==0.0.196 transformers\n",
    "# !pip install google-cloud-aiplatform==1.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically restart kernel after installs so that your environment\n",
    "# can access the new packages.\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "# Please fill in these values.\n",
    "project_id = \"engaged-plasma-439804-k0\"  # @param {type:\"string\"}\n",
    "database_password = os.getenv('DATABASE_PASSWORD')  # @param {type:\"string\"}\n",
    "region = \"us-central1\"  # @param {type:\"string\"}\n",
    "instance_name = \"research-paper-db\"  # @param {type:\"string\"}\n",
    "database_name = \"research-papers\"  # @param {type:\"string\"}\n",
    "database_user = \"superuser\"  # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# Quick input validations.\n",
    "assert project_id, \"⚠️ Please provide a Google Cloud project ID\"\n",
    "assert region, \"⚠️ Please provide a Google Cloud region\"\n",
    "assert instance_name, \"⚠️ Please provide the name of your instance\"\n",
    "assert database_name, \"⚠️ Please provide a database name\"\n",
    "assert database_user, \"⚠️ Please provide a database user\"\n",
    "assert database_password, \"⚠️ Please provide a database password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Verify that you are able to connect to the database. Executing this block should print the current PostgreSQL server version.\n",
    "\n",
    "import asyncio\n",
    "import asyncpg\n",
    "from google.cloud.sql.connector import Connector\n",
    "\n",
    "\n",
    "async def test_connection():\n",
    "    # get current running event loop to be used with Connector\n",
    "    print(1)\n",
    "    loop = asyncio.get_running_loop()\n",
    "    print(1)\n",
    "    # initialize Connector object as async context manager\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        print(1)\n",
    "        # create connection to Cloud SQL database\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\"\n",
    "            # ... additional database driver args\n",
    "        )\n",
    "\n",
    "        # query Cloud SQL database\n",
    "        results = await conn.fetch(\"SELECT version()\")\n",
    "        print(results[0][\"version\"])\n",
    "\n",
    "        # close asyncpg connection\n",
    "        await conn.close()\n",
    "\n",
    "\n",
    "# Test connection with `asyncio`\n",
    "await test_connection()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Pandas dataframe in a PostgreSQL table.\n",
    "\n",
    "import asyncio\n",
    "import asyncpg\n",
    "from google.cloud.sql.connector import Connector\n",
    "\n",
    "\n",
    "async def reset_database():\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await conn.execute(\"DROP TABLE IF EXISTS papers CASCADE\")\n",
    "        # Create the `products` table.\n",
    "        await conn.execute(\n",
    "            \"\"\"CREATE TABLE papers(\n",
    "                                doi TEXT PRIMARY KEY,\n",
    "                                title TEXT,\n",
    "                                abstract TEXT,\n",
    "                                content TEXT,\n",
    "                                citation_text TEXT,\n",
    "                                embedding vector(1024))\"\"\"\n",
    "        )\n",
    "\n",
    "        # Copy the dataframe to the `products` table.\n",
    "        # tuples = list(df.itertuples(index=False))\n",
    "        # await conn.copy_records_to_table(\n",
    "        #     \"products\", records=tuples, columns=list(df), timeout=10\n",
    "        # )\n",
    "        await conn.close()\n",
    "\n",
    "\n",
    "# Run the SQL commands now.\n",
    "# await reset_database()  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the generated vector embeddings in a PostgreSQL table.\n",
    "# This code may run for a few minutes.\n",
    "\n",
    "import asyncio\n",
    "import asyncpg\n",
    "from google.cloud.sql.connector import Connector\n",
    "import numpy as np\n",
    "from pgvector.asyncpg import register_vector\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "async def insert_into_papers(papers_df = None):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "        await register_vector(conn)\n",
    "\n",
    "\n",
    "        # res = await conn.execute(\"DROP TABLE IF EXISTS papers CASCADE\")\n",
    "        # print(res)\n",
    "        # Create the `products` table.\n",
    "        # await conn.execute(\n",
    "        #     \"\"\"CREATE TABLE papers(\n",
    "        #                         doi TEXT PRIMARY KEY,\n",
    "        #                         title TEXT,\n",
    "        #                         abstract TEXT,\n",
    "        #                         content TEXT,\n",
    "        #                         citation_text TEXT,\n",
    "        #                         embedding vector(1024))\"\"\"\n",
    "        # )\n",
    "        if type(df) == pd.DataFrame:\n",
    "            # Store all the generated embeddings back into the database.\n",
    "            insert_values = [\n",
    "                (\n",
    "                    str(row[\"doi\"]),\n",
    "                    row[\"title\"],\n",
    "                    row[\"abstract\"],\n",
    "                    row[\"content\"],\n",
    "                    row[\"citation_text\"],\n",
    "                    np.array(row[\"embedding\"]),\n",
    "                    row['id'],\n",
    "                    np.array(row['title_embedding']),\n",
    "\n",
    "                )\n",
    "                for index, row in papers_df.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Use executemany for bulk insert\n",
    "            await conn.executemany(\n",
    "                \"INSERT INTO papers (doi, title, abstract, content, citation_text, embedding, id, title_embedding) VALUES ($1, $2, $3, $4, $5, $6, $7, $8)\",\n",
    "                insert_values\n",
    "            )\n",
    "\n",
    "        await conn.close()\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# df = pd.read_csv(\"test.csv\")\n",
    "# df[\"embedding\"] = [np.random.rand(1024) for _ in range(len(df))]\n",
    "# df\n",
    "# # Run the SQL commands now.\n",
    "# await insert_into_papers(df)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def update_papers(papers_df = None):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await conn.execute(\"CREATE EXTENSION IF NOT EXISTS vector\")\n",
    "        await register_vector(conn)\n",
    "\n",
    "        if type(papers_df) == pd.DataFrame:\n",
    "            # Store all the generated embeddings back into the database.\n",
    "            update_values = [\n",
    "                (\n",
    "                    str(row[\"doi\"]),\n",
    "                    row[\"title_embedding\"],\n",
    "                    row[\"id\"],\n",
    "                )\n",
    "                for index, row in papers_df.iterrows()\n",
    "            ]\n",
    "\n",
    "            # Use executemany for bulk insert\n",
    "            await conn.executemany(\n",
    "                \"\"\"\n",
    "                UPDATE papers \n",
    "                SET title_embedding = $2, id = $3 \n",
    "                WHERE doi = $1\n",
    "                \"\"\",\n",
    "                update_values\n",
    "            )\n",
    "\n",
    "        await conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch(embeding, nbr_articles=2, DESC=True):\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await register_vector(conn)\n",
    "        # Find similar products to the query using cosine similarity search\n",
    "        # over all vector embeddings. This new feature is provided by `pgvector`.\n",
    "        results = await conn.fetch(\n",
    "            f\"\"\"\n",
    "                            SELECT title, 1 - (embedding <=> $1) AS similarity, abstract, doi, 1 - (title_embedding <=> $1) AS title_similarity\n",
    "                            FROM papers\n",
    "                            ORDER BY similarity {\"DESC\" if DESC else \"\"}\n",
    "                            LIMIT $2\n",
    "                            \"\"\",\n",
    "            embeding,\n",
    "            nbr_articles\n",
    "        )\n",
    "\n",
    "        if len(results) == 0:\n",
    "            raise Exception(\"Did not find any results. Adjust the query parameters.\")\n",
    "        matches = []\n",
    "        for r in results:\n",
    "            # Collect the description for all the matched similar toy products.\n",
    "            matches.append(\n",
    "                {\n",
    "                    \"title\": r[\"title\"],\n",
    "                    \"sim\": r[\"similarity\"],\n",
    "                    \"title_similarity\": r[\"title_similarity\"],\n",
    "                    \"abstract\": r[\"abstract\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        await conn.close()\n",
    "        return matches\n",
    "if False:\n",
    "    embeding = np.random.rand(768)\n",
    "\n",
    "    # Run the SQL commands now.\n",
    "    res = await fetch(embeding)  # type: ignore\n",
    "    res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "model_type = 'jina'\n",
    "if model_type == \"bert\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "    model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "elif model_type == \"roberta\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained('FacebookAI/roberta-base')\n",
    "    model = AutoModel.from_pretrained('FacebookAI/roberta-base')  \n",
    "elif model_type == 'jina':\n",
    "    tokenizer = AutoTokenizer.from_pretrained('jinaai/jina-embeddings-v3')\n",
    "    model = AutoModel.from_pretrained('jinaai/jina-embeddings-v3', trust_remote_code=True)\n",
    "else:\n",
    "    Exception(\"No model chosen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Sample text\n",
    "text = \"SciBERT is a pretrained transformer model for scientific text.\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Forward pass to get the embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract the last hidden states (embeddings)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Optionally, get the embeddings for the [CLS] token\n",
    "cls_embedding = last_hidden_states[:, 0, :]\n",
    "\n",
    "# Print the shape of the embeddings\n",
    "print(\"Shape of last hidden states:\", last_hidden_states.shape)\n",
    "print(\"Shape of CLS embedding:\", cls_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv(\"test.csv\")\n",
    "df[\"embedding\"] = [np.random.rand(768) for _ in range(len(df))]\n",
    "with torch.no_grad():\n",
    "    embeds = model(**tokenizer(df[\"abstract\"].to_list(), return_tensors='pt', padding=True, truncation=True)).last_hidden_state[:, 0, :]\n",
    "    for idx, emb in enumerate(embeds):\n",
    "        df.at[idx, \"embedding\"] = emb.numpy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# await reset_database() #[-0.1714102, 0.548595, -0.22505158, 0.003960\n",
    "# await insert_into_papers(df) #[-0.1714096, 0.5485956, -0.22505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_all():\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await register_vector(conn)\n",
    "        # Find similar products to the query using cosine similarity search\n",
    "        # over all vector embeddings. This new feature is provided by `pgvector`.\n",
    "        results = await conn.fetch(\n",
    "            \"\"\"\n",
    "                            SELECT *\n",
    "                            FROM papers\n",
    "                            \"\"\"\n",
    "        )\n",
    "\n",
    "        if len(results) == 0:\n",
    "            return []\n",
    "        matches = []\n",
    "        for r in results:\n",
    "            # Collect the description for all the matched similar toy products.\n",
    "            matches.append(\n",
    "                {\n",
    "                    \"doi\": r[\"doi\"],\n",
    "                    \"title\": r[\"title\"],\n",
    "                    \"abstract\": r[\"abstract\"],\n",
    "                    \"content\": r[\"content\"],\n",
    "                    \"emb\": r[\"embedding\"],\n",
    "                    \"id\": r[\"id\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        await conn.close()\n",
    "        return matches\n",
    "matches = await fetch_all()\n",
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstract = pd.read_csv(\"./data/ml_100_abstracts.csv\")\n",
    "df_abstract[\"doi\"] = None\n",
    "df_abstract[\"title\"] = None\n",
    "df_abstract[\"content\"] = \"None\"\n",
    "df_abstract[\"citation_text\"] = \"None\"\n",
    "for i in range(len(df_abstract)):\n",
    "    df_abstract.loc[i, \"doi\"] = \"paper\" + str(i)\n",
    "    df_abstract.loc[i, \"title\"] = str(i)\n",
    "    df_abstract.loc[i, \"content\"] = str(i)\n",
    "    df_abstract.loc[i, \"citation_text\"] = str(i)\n",
    "df_abstract.rename(columns={'Abstract': 'abstract'}, inplace=True)\n",
    "df_abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def add_embedings(df: pd.DataFrame, model, tokenizer, field_to_embedd=\"abstract\", embedding_field_name=\"embedding\"):\n",
    "    # df[\"embedding\"] = None\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(df[field_to_embedd].to_list(), return_tensors='pt', padding=True, truncation=True)\n",
    "        embeds = model(**inputs, task=\"retrieval.passage\").last_hidden_state[:, 0, :]\n",
    "        df.insert(len(df), embedding_field_name, embeds.tolist())\n",
    "        # assert len(df) == len(embeds)\n",
    "        # for idx, emb in enumerate(embeds):\n",
    "        #     df.at[idx, \"embedding\"] = emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset_database() #[-0.1714102, 0.548595, -0.22505158, 0.003960\n",
    "# await insert_into_papers(df_abstract) #[-0.1714096, 0.5485956, -0.22505"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"summarize the papers related to linear regression\",\n",
    "        \"quantum mechanics is a very interesting field plese get mo som epapers related to that subject\",\n",
    "        \"What are transformers what are examples of transformer, encoder decoder models\"\n",
    "        ]\n",
    "\n",
    "async def get_similar(text, nbr_articles=5):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Forward pass to get the embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, task=\"retrieval.query\")\n",
    "\n",
    "    # Extract the last hidden states (embeddings)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Optionally, get the embeddings for the [CLS] token\n",
    "    cls_embedding = last_hidden_states[:, 0, :]\n",
    "    return await fetch(cls_embedding[0], nbr_articles=nbr_articles, DESC=True)\n",
    "\n",
    "for sent in text:\n",
    "    print(sent)\n",
    "    res = await get_similar(sent)\n",
    "    for i in res:\n",
    "        print(i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = pd.read_json(\"./data/ml_papers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json_filterd = df_json[['doi', 'title', 'abstract', 'id']]\n",
    "df_json_filterd[\"content\"] = \"None\"\n",
    "df_json_filterd[\"citation_text\"] = \"None\"\n",
    "df_json_filterd = df_json_filterd.drop_duplicates(subset='doi', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# from cryptography.utils import CryptographyDeprecationWarning \n",
    "# warnings.filterwarnings(\"ignore\", category=CryptographyDeprecationWarning)\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "dois = [item['doi'] for item in await fetch_all()]\n",
    "print(dois)\n",
    "chunk_size = 5\n",
    "seq = df_json_filterd[~df_json_filterd['doi'].isna()][5000:10000]\n",
    "seq = seq[~seq['doi'].isin(dois)]\n",
    "for idx, chunk in enumerate(chunker(seq=seq, size=chunk_size)):\n",
    "    print(\"processing \", idx*chunk_size, \"to\", (idx+1)*chunk_size)\n",
    "    add_embedings(chunk, model=model, tokenizer=tokenizer)\n",
    "    add_embedings(chunk, model=model, tokenizer=tokenizer, field_to_embedd=\"title\", embedding_field_name=\"title_embedding\")\n",
    "    await insert_into_papers(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "dois = [item['doi'] for item in await fetch_all() if item['id'] == None]\n",
    "print(len(dois))\n",
    "chunk_size = 5\n",
    "seq = df_json_filterd[~df_json_filterd['doi'].isna()][5000:10000]\n",
    "seq = seq[seq['doi'].isin(dois)]\n",
    "for idx, chunk in enumerate(chunker(seq=seq, size=chunk_size)):\n",
    "    print(\"processing \", idx*chunk_size, \"to\", (idx+1)*chunk_size)\n",
    "    add_embedings(chunk, model=model, tokenizer=tokenizer, field_to_embedd=\"title\", embedding_field_name=\"title_embedding\")\n",
    "    await update_papers(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_all_citations():\n",
    "    loop = asyncio.get_running_loop()\n",
    "    async with Connector(loop=loop) as connector:\n",
    "        # Create connection to Cloud SQL database.\n",
    "        conn: asyncpg.Connection = await connector.connect_async(\n",
    "            f\"{project_id}:{region}:{instance_name}\",  # Cloud SQL instance connection name\n",
    "            \"asyncpg\",\n",
    "            user=f\"{database_user}\",\n",
    "            password=f\"{database_password}\",\n",
    "            db=f\"{database_name}\",\n",
    "        )\n",
    "\n",
    "        await register_vector(conn)\n",
    "        # Find similar products to the query using cosine similarity search\n",
    "        # over all vector embeddings. This new feature is provided by `pgvector`.\n",
    "        results = await conn.fetch(\n",
    "            \"\"\"\n",
    "            SELECT *\n",
    "            FROM citations\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        if len(results) == 0:\n",
    "            return []\n",
    "        matches = []\n",
    "        for r in results:\n",
    "            # Collect the description for all the matched similar toy products.\n",
    "            matches.append(\n",
    "                {\n",
    "                    \"source_paper\": r[\"source_paper\"],\n",
    "                    \"cited_by\": r[\"cited_by\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        await conn.close()\n",
    "        return matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(bois: list[str], citations: list[dict], max_papers=10) -> list[str]:\n",
    "    \"\"\"\n",
    "    Performs a breadth-first search (BFS) to explore related papers based on citations.\n",
    "\n",
    "    This function takes a list of base of interest (BOI) papers and a list of citation data to find\n",
    "    additional papers that are cited by or cite the BOI papers. The search continues until it reaches \n",
    "    a specified maximum number of papers.\n",
    "\n",
    "    Parameters:\n",
    "    - bois (list[str]): A list of DOIs (or identifiers) representing the base papers of interest.\n",
    "    - citations (list[dict]): A list of citation records, where each record is a dictionary containing:\n",
    "        - 'cited_by' (str): The DOI of the paper that is citing another paper.\n",
    "        - 'source_paper' (str): The DOI of the paper being cited.\n",
    "    - max_papers (int): The maximum number of papers to return, including the base papers and \n",
    "                        their related papers. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "    - list[str]: A list of DOIs representing the base papers and any additional papers found \n",
    "                 through citations, up to the maximum specified.\n",
    "\n",
    "    Note:\n",
    "    The search stops once the total number of identified papers reaches the `max_papers` limit.\n",
    "    \"\"\"\n",
    "    based_on = set()\n",
    "    future_work = set()\n",
    "    queue = [i for i in bois]\n",
    "    for boi in queue:\n",
    "        for citation in citations:\n",
    "            if citation['cited_by'] == boi and citation['source_paper'] not in based_on:\n",
    "                print(citation, \"by\")\n",
    "                queue.append(citation['source_paper'])\n",
    "                based_on.add(citation['source_paper'])\n",
    "            if len(bois) + len(based_on) >= max_papers: break\n",
    "        if len(bois) + len(based_on) >= max_papers: break\n",
    "\n",
    "    queue = [i for i in bois]\n",
    "    for boi in queue:\n",
    "        for citation in citations:\n",
    "            if citation['source_paper'] == boi  and citation['cited_by'] not in future_work:\n",
    "                print(citation, \"source\")\n",
    "                queue.append(citation['cited_by'])\n",
    "                future_work.add(citation['cited_by'])\n",
    "            if len(bois) + len(based_on) + len(future_work) >= max_papers: break\n",
    "        if len(bois) + len(based_on) + len(future_work) >= max_papers: break\n",
    "    return bois + list(based_on) + list(future_work)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfs([\"10.1109/ICASSP.2019.8683438\"], await fetch_all_citations()), set(bfs([\"10.1109/ICASSP.2019.8683438\"], await fetch_all_citations()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
